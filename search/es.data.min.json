[{"id":0,"href":"/overview/","title":"Información","parent":"Inicio","content":"    Primeros Pasos      Sobre Khipu      Infraestructura      Galería      "},{"id":1,"href":"/cuentas/","title":"Cuentas","parent":"Inicio","content":"    Grupos de Cuentas      Solicitud de Acceso      Políticas      "},{"id":2,"href":"/overview/primeros_pasos/","title":"Primeros Pasos","parent":"Información","content":"    Clusters HPC Khipu Solicitar una cuenta Iniciar Sesión Enviar un Job Linux Enviar Archivos Uso de Software Reglas de uso Obtener ayuda      Clusters HPC Khipu     Visto de manera sencilla, Khipu es una colección de computadoras que que se encuentran conectadas entre sí y que trabajan como si fueran una sola. A cada computador conectado se le conoce como nodo. El nodo de acceso, o también llamado nodo líder, es aquel al cual nos conectamos de manera remota para acceder al cluster. Los nodos que se encargan de la ejecución de nuestros jobs o trabajos, son llamados nodos de cómputo. Para enviar un job hacemos uso de un scheduler, el cual se encargado de manejar los diferentes jobs que se envían y garantizar equidad al momento que pasan a ejecutarse. Todos los nodos de computo, poseen un filesystem compartido lo cual permite a los jobs acceder y modificar la data de cualquier nodo sin la necesidad de copiarlo al otro.\nSolicitar una cuenta     Para poder acceder al cluster Khipu es necesario solicitar una cuenta. Las cuentas se dividen en dos grupos: educación e investigación. Las solicitudes de cuentas no implican costo alguno para los miembros de UTEC y pueden ser solicitadas por el docente del curso o el encargado del proyecto de investigación. Mayores detalles sobre las cuentas en Khipu lo encuentra aquí.\nAbrir solicitud de acceso    Una vez completado el formulario y aprobada la solicitud de acceso, las credenciales serán enviadas a los correos registrados. A partir del momento en que se reciben las credenciales, el usuario se compromete a cumplir las políticas de uso.\nIniciar Sesión     Una vez adquirida una cuenta, puede acceder al cluster por terminal via SSH. Diríjase al apartado de inicio de sesión para conocer cómo realizar este proceso.\nEnviar un Job     Dentro del cluster, usted puede enviar a ejecución sus trabajos o jobs a través de un scheduler. Una vez adquirida una cuenta, puede acceder al cluster por terminal via SSH. Diríjase al apartado de enviar jobs para conocer cómo realizar este proceso.\nLinux     Enviar Archivos     Uso de Software     Reglas de uso     Obtener ayuda     "},{"id":3,"href":"/anuncios/","title":"Anuncios","parent":"Inicio","content":""},{"id":4,"href":"/cuentas/cuentas_del_cluster/","title":"Grupos de Cuentas","parent":"Cuentas","content":"Existen dos grupos de usuarios gerenciados automáticamente por Slurm: 1) educación, y 2) investigación. Ambos grupos acceden al cluster por el nodo líder para enviar trabajos a la fila de ejecución.\nGrupo Educación     Permite el uso del cluster para aula o laboratorio por los estudiantes a pedido de un instructor(a) registrado(a). Este grupo es financiado por la universidad y sus trabajos serán procesados bajo las siguientes características:\n Ocupar un nodo CPU y/o GPU. Equivalente a un total de 72 cores, 320 GB de memoria y 1.4 TB de almacenamiento.\n Tiempo máximo de ejecución de trabajo: 2 horas.\n Número de procesos concurrentes: 2.\n Cantidad de cores por proceso 8.  Grupo Investigación     Este grupo está financiado por UTEC, proyectos de investigación (PI), Departamentos y Dirección de Escuela. Los fondos centrales cubren costos de infraestructura, operación y soporte. Los PI y algunas unidades y departamentos financian la adquisición de nuevos nodos de procesamiento y almacenamiento. Los usuarios de este grupo ejecutan trabajos en todos los nodos bajo las siguientes características:\n Ocupar cualquier nodo CPU y/o GPU. Equivalente a un total de 200 cores, 1.1 TB de memoria y 5.3 TB de almacenamiento.\n Tiempo máximo de ejecución de trabajo NaN.\n Número de procesos concurrentes NaN.\n Cantidad de cores por proceso NaN.  "},{"id":5,"href":"/cuentas/solicitud_de_acceso/","title":"Solicitud de Acceso","parent":"Cuentas","content":"Para poder obtener credenciales de acceso a Khipu, es necesario completar un formulario con información relevante del proyecto o curso. Este formulario deberá ser completado por: a) el investigador principal del proyecto (PI), o b) el profesor responsable del curso.\nEl formulario es ligeramente distinto para cada grupo de cuentas. Sin embargo, en ambos casos se deberá completar el nombre del proyecto o curso, la fecha de inicio y fin, el investigador o docente a cargo, los miembros del curso o proyecto y la lista de software licenciado o de código abierto a emplearse. Esta última información nos es útil para poder identificar los softwares con mayor demanda y así crear los módulos correspondientes.\nPuede acceder a la solicitud acceso a través del siguiente enlace:\nAbrir solicitud de acceso    También se puede acceder al formulario haciendo click a la opción Registro dentro de https://web.khipu.utec.edu.pe/\n Una vez completado el formulario, se obtendrá respuesta a la solicitud en un plazo no mayor de 2 días hábiles. Si la solicitud es favorable, también se realizará el envío de las credenciales de acceso a los correos electrónicos de cada miembro registrado. Una vez que un usuario recibe sus credenciales se compromete a cumplir con las políticas de uso. Por otro lado, si la solicitud no es favorable, se enviará por correo las razones de la decisión.\n"},{"id":6,"href":"/cuentas/politica/","title":"Políticas","parent":"Cuentas","content":"    Sobre las cuentas Sobre el uso Incumplimiento de las reglas de uso Agradecimientos por el uso de Khipu Comunicación      El uso del cluster HPC Khipu esta sujeto al cumplimiento de sus políticas de uso. Una vez que se reciben las credenciales de acceso, los usuarios aceptan cumplir las siguiente políticas.\nSobre las cuentas      Solo los usuarios registrados pueden acceder a Khipu. Asimismo, los usuarios se hacen responsables de tomar las precauciones necesarias para proteger sus credenciales de acceso y así impedir accesos no autorizados. Los usuarios se encuentran prohibidos de compartir sus credenciales con otras personas, así sean estos estudiantes o colaboradores. Del mismo modo, los usuarios se encuentran prohibidos de acceder a Khipu con credenciales que no son suyas, con o sin consentimiento del usuario. En caso de existir sospecha que otros han usado su cuenta, notificarlo inmediatamente a khipu@utec.edu.pe. Las cuentas serán desactivadas si una de las siguientes condiciones se cumple:  La fecha de fin del curso o proyecto ha sido alcanzada, o El encargado del curso o proyecto indica que el usuario ya no requiere acceso, o El usuario ha recibido alguna sanción disciplinaria grave de parte de la universidad.    Sobre el uso     El cluster Khipu es un recurso compartido por multiples usuarios, así que las acciones que usted realice pueden afectar a otros usuarios si no se realizan de manera adecuada. Es por ello, que se brindan las siguientes recomendaciones a fin de garantizar un uso equitativo y justo de los recursos del cluster.\n El uso del cluster se encuentra restringido solo para fines educativos y de investigación. Su uso no debe estar relacionado a actividades comerciales, de consultoría o creación de software malicioso. Los usuarios son responsables de usar los recursos del cluster de manera eficiente, efectiva, ética y lícita.   El nodo de administración es usado para acceder al cluster, editar archivos, compilar código y enviar trabajos. De ninguna manera, se deben ejecutar los trabajos directamente en el nodo de administración. Toda ejecución de trabajos debe ser realizada a traves del gestor de recursos slurm. Es así como se garantizan un uso eficiente y justo de los recursos. El cluster no debe ser utilizado como almacenamiento personal. Se recomienda copiar sus resultados a su máquina personal una vez culminado el uso del cluster. Los usuarios no deben intentar acceder a cualquier archivo o programa al cual no poseen autorización o un consentimiento explicito del dueño del archivo o programa. Los usuarios pueden hacer uso de las aplicaciones ya instaladas, y también compilar nuevas aplicaciones en su espacio personal solamente si fueran necesarios para el desarrollo de su proyecto o trabajo. Los softwares que se instalen deben incluir una licencia válida (si es aplicable). No se puede instalar software de procedencia ilegal y/o con licencia no otorgada por los desarrolladores. La presente política será revisada y actualizada de manera periódica. Cualquier cambio será comunicado al correo electrónico registrado.  Incumplimiento de las reglas de uso     El usuario acepta cumplir con la normativa y sanciones impuestas por UTEC en sus atribuciones.\n En caso de no cumplir normas de UTEC y/o atentar contra la ley, el evento será notificado a las autoridades pertinentes. En caso de incumplimiento de los items anteriores, la cuenta será suspendida.  Agradecimientos por el uso de Khipu     Alentamos a todos a los usuarios cuyo proyecto termina en una publicación o presentación a añadir al cluster Khipu dentro de los agradecimientos. Nos ayudaría de sobremanera, mencionar el rol que tuvo Khipu a lo largo de su investigación o formación académica. Su sola mención contribuye a comunicar el rol de Khipu en el desarrollo de investigaciones dentro de la universidad, incentiva a más estudiantes y docentes a incorporar a Khipu dentro de sus proyectos o planes de estudio, mantiene el financiamiento y soporte que hace posible que Khipu siga creciendo cada día más. Puede añadir un mensaje de agradecimiento como el siguiente:\n \u0026ldquo;El trabajo computacional del presente producto fue apoyado por los recursos del cluster HPC Khipu (https://web.khipu.utec.edu.pe/) de la Universidad de Ingeniería y Tecnología (UTEC) \u0026quot;\n Comunicación     El correo electrónico ingresado al momento de solicitar acceso a Khipu será el medio por el cual se enviarán anuncios relevantes al funcionamiento del cluster. Si usted presenta preguntas, pedidos o requiere asistencia puede escribir de manera directa a khipu@utec.edu.pe. Por favor, asegúrese de seguir las recomendaciones de soporte a fin de poder atender sus requerimientos de mejor manera.\n"},{"id":7,"href":"/cuentas/recomendaciones/","title":"Recomendaciones","parent":"Cuentas","content":""},{"id":8,"href":"/guia_de_usuario/","title":"Guía de Usuario","parent":"Inicio","content":"    Iniciar Sesión      Software      Software Disponible      Usando Software      Instalando Software        Ejecutando Jobs      Sobre Slurm      Ejecutar y Revisar Jobs      Ejemplos de Jobs Scripts      Monitorear Jobs      Ejemplos de Scheduler     Configuración de Scheduler       Transfiriendo Data     Configuracion de Hardware     "},{"id":9,"href":"/overview/khipu/","title":"Sobre Khipu","parent":"Información","content":"    Sobre Khipu  ¿Qué es computación de alto desempeño?        Sobre Khipu     Khipu es un cluster de computación de alto desempeño que forma Centro de Investigación para la Computación Sostenible (COMPSUST) y el Departamento de Ciencia de la Computación (CS) de la Universidad de Ingeniería y Tecnología (UTEC).\n Misión\nSomos un grupo académico que apostamos por el desarrollo científico de nuestra comunidad, a traves de la computación de alto desempeño.\n   Visión\nSer un laboratorio referente en computación de alto desempeño a nivel local, y ser reconocidos por nuestra colaboración al desarrollo científico de nuestra comunidad.\n   ¿Qué es computación de alto desempeño?     La computación de alto desempeño, o HPC por sus siglas en inglés, es la práctica de agregar poder computacional de diferentes ordenadores, de tal manera que se obtiene un desempeño mucho mayor al de un ordenador convencional aprovechando el poder del paralelismo. Todo ello con el objetivo de resolver de manera más rápida los problemas complejos de campos como matemáticas, ciencias e ingenierías.\nHPC nos permite resolver problemas complejos que tardarían años, meses o semanas para resolverse en un computador convencional, pero que con el uso de HPC, se podría reducir a días, horas o minutos.\nLeer más:\n What is High Performance Computing?  What is High-Performance Computing (HPC)?  "},{"id":10,"href":"/guia_de_usuario/iniciar_sesion/","title":"Iniciar Sesión","parent":"Guía de Usuario","content":"    Inicio de sesión regular Inicio de sesión con SSH Keys  Linux o MacOS Windows        Inicio de sesión regular     Para poder iniciar sesión en Khipu deberemos seguir los siguientes pasos.\n Abrir un terminal y escribir el siguiente comando reemplazando su nombre de usuario en el campo .  ssh \u0026lt;username\u0026gt;@khipu.utec.edu.pe Ejemplo:\nssh juana.perez@khipu.utec.edu.pe Aparecerá el siguiente diálogo. Aceptar escribiendo yes y presionar enter. Luego colocar el password (este no aparecerá en pantalla al escribir) y presionar enter.  Are you sure you want to continue connecting (yes/no)? juana.perez@khipu.utec.edu.pe\u0026#39;s password: Al lograr accesar al cluster aparecerá el prompt del terminal remoto.  Last login: Fri Aug 21 11:28:13 2020 from 190.236.197.233 [juanaperez@khipu ~]$ Para cerrar o salir del cluster:  [juana.perez@khipu ~]$ exit Inicio de sesión con SSH Keys     Sobre las SSH Keys ↕  ¿Qué son las SSH Keys?     SSH o Secure Shell Keys son un conjunto de información que se emplea para identificar y encriptar la comunicación entre su máquina local y un servidor. Se compone de dos archivos: una clave pública (id_rsa.pub) y otra privada (id_rsa o id_rsa.ppk). Se manera simple la clave pública es un \u0026ldquo;candado\u0026rdquo; y la clave privada es la llave para abrir dicho \u0026ldquo;candado\u0026rdquo;. El \u0026ldquo;candado\u0026rdquo; (llave pública) puede ser compartida sin mayores problemas, en cambio si comparte su llave privada, alguien podría suplantar su identidad.\nCuando se conecta a un servidor, este presentará su llave pública. Usted provará su identidad mediante el uso de su llave privada. La comunicación con el servidor remoto y toda la información enviado durante este proceso será encriptada mediante su llave pública, de tal manera que solo usted pueda desencriptarla usando su llave privada.\n  Si desea acceder a Khipu sin escribir su contraseña deberá copiar la llave pública de su computador al cluster. La llave pública debe haber sido creada previamente con el siguiente comando.\nssh-keygen Su terminal responderá con:\nGenerating public/private rsa key pair. Enter file in which to save the key (/home/username/.ssh/id_rsa): Presione Enter para aceptar el valor por defecto. Su terminal responderá con:\nEnter passphrase (empty for no passphrase): Usted deberá eligir un passphrase seguro. Este valor le ayudará a prevenir el acceso a su cuenta en caso su clave privada es robada. Mientras escribe su passphrase este no aparecerá en pantalla. Su terminal responderá con:\nEnter same passphrase again: Ingrese el passphrase nuevamente. Una vez hecho esto se generará el par de llaves dentro de un directorio .ssh en su directorio /home. Si usted olvida su passphrase, no podrá recuperarlo. En cambio, usted deberá generar y copiar al servidor un nuevo par de SSH Keys.\nA continuación se muestra como copiar su clave pública a Khipu desde diferentes sistemas operativos:\nLinux o MacOS     ssh-copy-id -i ~/.ssh/id_rsa.pub juana.perez@khipu.utec.edu.pe Windows     type %HOMEPATH%\\.ssh\\id_rsa.pub | ssh juana.perez@khipu.utec.edu.pe \u0026#34;mkdir .ssh; cat \u0026gt;\u0026gt; .ssh/authorized_keys\u0026#34; "},{"id":11,"href":"/guia_de_usuario/software/","title":"Software","parent":"Guía de Usuario","content":"    Software Disponible      Usando Software      Instalando Software      "},{"id":12,"href":"/overview/infraestructura/","title":"Infraestructura","parent":"Información","content":"    Vista general Nodo de administración Nodos de computación  Nodos CPU Nodos GPU   Software del sistema Aplicaciones de software      Vista general      .myTable { border-radius: 5px; } .myTable th { background-color:var(--header-background); color: white; }  Nodo de administración     Nodo de acceso al cluster, usado principalmente para compilar y enviar trabajos.\n   Especificaciones     Nombre nLíder   Procesador Intel(R) Xeon(R) Gold 6230 CPU @ 2.10 GHz 20 cores por socket, 40 por nodo.    Memoria DRAM DDR4-1333 MHz, 128 GB por nodo    Almacenamiento 480 SSD, total 40 TB HDD    Red Infiniband FDR MT4119    Nodos de computación     Usado para procesamiento, gerenciado automáticamente por Slurm.\nNodos CPU        Especificaciones     Nombre nCPU   Procesador Intel(R) Xeon(R) Gold 6130 CPU @2.10 GHz 16 cores por socket, 32 por nodo.   Memoria DRAM DDR4-1333 MHz, 128 GB por nodo    Almacenamiento 960 GB SSD   Red Infiniband FDR MT4119    Nodos GPU        Especificaciones     Nombre nGPU   Procesador Intel(R) Xeon(R) Gold 6230 CPU @2.10 GHz 20 cores por socket, 40 por nodo.    Gráficos NVIDIA Tesla T4 16 GB GDDR6, PCIe 3.0 x16 1 GPU por nodo.   Memoria  DRAM DDR4-1333 MHz, 128 GB por nodo    Almacenamiento 480 GB SSD   Red Infiniband FDR MT4119     Software del sistema      Sistema Operativo: CentOS Linux 7 Message Passing Library: MPICH Compiladores: Intel, GCC Job Scheduler: SLURM Manejo de software: Módulos de ambiente  Aplicaciones de software     Revise aquí la lista de softwares, herramientas y utilidades disponible en Khipu.\n"},{"id":13,"href":"/guia_de_usuario/software/software_disponible/","title":"Software Disponible","parent":"Software","content":"    Módulos disponibles      Para poder satisfacer las diferentes necesidades de nuestros usuarios, hacemos uso de sotfware modules a fin de poder hacer disponibles multiples versiones de software. Los módulos ayudan a cambiar entre diferentes aplicaciones y sus versiones con relativa facilidad, especialmente en ambientes compartidos.\nMódulos disponibles     Usted puede listar los módulos que se encuentran disponibles con el siguiente comando.\nmodule avail A continuación se comparte una muestra de la salida del comando anterior.\n--------------------------------------- /opt/apps/modulefiles ---------------------------------------- armadillo/10.8.2 go/1.17.6 mpich/4.0 brams/5.3 gromacs/2020.1 namd/2019.07.11-multicore cmake/3.16.5 gromacs/2020.3 namd/2022.04.05-multicore cmake/3.22.2 gromacs/gpu-2020.1 namd/2022.04.05-multicore-CUDA cuda/10.2 gromacs/gpu-2020.3 openfoam/v2112 cuda/11.4 intel-oneapi/2022.1.2 pmix/2.1 gaussian/1.1.0 julia/1.5.3 python/2.7.17 gcc/10.1.0 meep/1.17.1 python/3.7.7 gcc/5.5.0 metis/5.1.0 python/3.9.2 gcc/6.5.0 mpich/1.5 speedtest/latest gcc/7.5.0 mpich/3.1.4 telemac/8.3.0 gcc/8.4.0 mpich/3.2.1 valgrind/3.15.0 gcc/9.2.0 mpich/3.3.2 valgrind/3.18.1 go/1.14.6 mpich/3.4.0 wrf/4.2 A continuación se muestra la lista de software disponible por categorías.\n .myTable { border-radius: 5px; } .myTable th { background-color:var(--header-background); color: white; }     Categoría Componente     Sistema Operativo CentOS Linux 7.6 x86_64   Gestor de trabajos Slurm   Herramientas de desarrollo   Editores de texto Vim, Nano   Sistema de Control de Versiones Git   Compiladores GNU (gcc, g++, gfortran), CMake, Intel    Lenguajes C, C++, Python, R, Java, Go, Julia   Debugging, profiling, tracing GDB, CGDB, Valgrind   Distribución MPI MPICH   GPU CUDA   Otros   Ingeniería OpenFOAM   Geo-sotfware WRF, BRAMS, Telemac   Bioinformática NAMD    "},{"id":14,"href":"/guia_de_usuario/software/usando_software/","title":"Usando Software","parent":"Software","content":"    Módulos  Buscar módulos Cargar módulos Descargar módulos Colección de módulos Obtener información y ayuda de los módulos        Módulos     Buscar módulos     Usted también puede buscar módulos mediante el comando avail. Por ejemplo, para listar todas las versiones de MPICH 3, ejecutaremos:\nmodule avail mpich/3 Y obtendremos una salida similar a:\n--------------------------------------- /opt/apps/modulefiles ---------------------------------------- mpich/1.5 mpich/3.1.4 mpich/3.2.1 mpich/3.3.2 mpich/3.4.0 mpich/4.0 Cargar módulos     Usted puede cargar un modulo a su ambiente de trabajo a traves del comando module load. Al cargar un modulo, usted carga todas las variables de ambiente necesarias para poder utilizar dicho paquete de software.\nEs importante tomar en cuenta que:\n El comando es case-sensitive a los nombres de los módulos Cuando usted usa module load se cargan también las dependencias del modulo. No es necesario cargarlas de manera separada. Cuando usted enviá sus jobs que requieren de un módulo, no olvide añadirlo con module load a su bash script.  Por ejemplo, si desea cargar gcc versión 7.5.0, deberá ejecutar:\nmodule load gcc/7.5.0 También puede cargar varios módulos al mismo tiempo:\nmodule load gcc/7.5.0 mpich/3.3.2 Descargar módulos     Una vez terminado de usar el software, es recomendable hacer module unload \u0026lt;modulename\u0026gt;. Este proceso también se debe realizar e los scripts de los jobs.\nmodule unload gcc/7.5.0 También puede descargar todos los módulos a la vez con:\nmodule purge Colección de módulos     En casos cuando se trabaja con gran cantidad de módulos, puede ser molestoso estar cargando cada uno de ellos. Ante esto existe la opción de añadir dicho módulos a una colección y de este modo solo cargar la colección en lugar que todos los módulos de manera individual.\nGuardar colecciones\nPara ello, usted deberá cargar previamente los módulos que desea añadir a la colección y despues ejecutar.\nmodule save \u0026lt;collection-name\u0026gt; Cargar colecciones\nUsted podrá cargar la colección creada con:\nmodule restore \u0026lt;collection-name\u0026gt; Listar colecciones\nPara listar las colecciones creadas:\nmodule savelist Obtener información y ayuda de los módulos     Si desea mostrar la información de configuración de un modulo en especifico:\nmodule show \u0026lt;modulename\u0026gt; #or module display \u0026lt;modulename\u0026gt; Usted puede obtener una breve descripción sobre un módulo ejecutando:\nmodule help \u0026lt;modulename\u0026gt;/\u0026lt;version\u0026gt; Si usted no encuentra un paquete y considera que su instalación como modulo podría ser beneficiosa para otros usuarios más, puede escribirnos a khipu@utec.edu.pe para solicitar su instalación.\n  "},{"id":15,"href":"/guia_de_usuario/software/instalando_software/","title":"Instalando Software","parent":"Software","content":"    Requisitos para instalar software Proceso de instalación      El siguiente documento describe como instalar software en Khipu para un proyecto o curso. Antes de realizar una instalación, por favor revise si el software ya se encuentra disponible como módulo con el comando module avail\nRequisitos para instalar software      En caso de software licenciado, el usuario debe proveer la licencia y/o instalador del software al administrador para ser instalado. El usuario puede instalar el software y libraries necesarios en su /home/\u0026lt;username\u0026gt;/, en caso de no requerir licencia o ser de código abierto. El usuario puede compilar en el nodo líder antes de enviar un job a ejecutar. No enviar jobs de compilación a la fila del cluster. Siendo Khipu un cluster enfocado al procesamiento y HPC; no se instalarán bases de datos, containers o máquinas virtuales. Si el software o library requiere permisos especiales para instalación, solamente el profesor encargado del curso o un investigador de proyecto pueden solicitar la instalación bajo coordinación con el administrador.   Si el software que planea instalar posee dependencias como librerías, interfaces o módulos; revise primero si ya se encuentran disponibles en el cluster antes de instalarlas por su cuenta.  Para entrar en contacto con el administrador del cluster, por favor enviar un mensaje a khipu@utec.edu.pe.\nPor favor entrar en contacto en caso se requiera un software o library adicional que va a ser usado por múltiples investigadores o participantes de un curso que no ha sido especificado en el Formulario de Solicitud de Acceso. -- Proceso de instalación     Tal como se mencionó anteriormente, usted puede instalar software libre o que no requiere licencia en su espacio personal de trabajo. Sin embargo, antes de proceder con la instalación asegúrese que dicho software no requiere permisos root para ser instalado. Una vez echo esto, usted deberá moverse al directorio en el cual planea realizar la instalación y seguir las instrucciones del proveedor del software que planea instalar.\nPor lo general, los pasos son similares, es por ello que a continuación se muestra un ejemplo realizando la instalación de la librería Zlib, un software usado para comprimir y descomprimir data.\n Creamos un directorio en el cual almacenaremos los códigos fuente y binarios de las apps que vamos a instalar.  mkdir myApps cd myApps APPS_DIR=$(pwd) mkdir sources # source files mkdir apps # binary files  Entraremos al directorio sources/ y descargaremos el código fuente del paquete desde su sitio web.  cd $APPS_DIR/sources wget https://www.zlib.net/zlib-1.2.12.tar.xz  Descomprimimos el archivo descargado.  tar -xvf zlib-1.2.12.tar.xz  Entramos al nuevo directorio con los contenidos del paquete que acabamos de descomprimir.  cd zlib-1.2.12/  Cargamos el módulo del compilador para poder crear los binarios a partir del código fuente. En este caso cargaremos GCC 7.5.0, sin embargo es recomendable seguir los requisitos de cada software.  module load gcc/7.5.0  Por lo general, muchos código fuentes se compilan usando simplemente make y make install. Sin embargo, nosotros debemos configurar previamente el lugar donde se guardaran los binarios, ya que si no hacemos esto, se guardaran en los directorios /usr/lib a los cuales no tenemos acceso. Por lo general, bastará con configurar el argumento --prefix.  ./configure --prefix=$APPS_DIR/apps/zlib Para mayor información de como configurar el path de instalación, es recomendable seguir la documentación del software a instalar.\n Compilamos el código fuente.  make  Instalamos el software.  make install # copia los binarios a --prefix  Si nos movemos al directorio donde se encuentra la instalación, encontraremos por lo general una lista de directorios como la siguiente.  cd $APPS_DIR/apps/zlib ls # salida del comando anterior include lib share  En esos directorios se encuentran los binarios con los cuales podremos hacer uso del software.  "},{"id":16,"href":"/guia_de_usuario/ejecutando_trabajos/","title":"Ejecutando Jobs","parent":"Guía de Usuario","content":"    Sobre Slurm      Ejecutar y Revisar Jobs      Ejemplos de Jobs Scripts      Monitorear Jobs      Ejemplos de Scheduler     Configuración de Scheduler     "},{"id":17,"href":"/overview/galeria/","title":"Galería","parent":"Información","content":"     Vista frontal del cluster HPC Khipu        Nodos del cluster HPC Khipu        Racks disponibles del cluster HPC Khipu        Vista de todos los nodos del cluster HPC Khipu        Vista general del cluster HPC Khipu en UTEC    "},{"id":18,"href":"/guia_de_usuario/ejecutando_trabajos/sobre_slurm/","title":"Sobre Slurm","parent":"Ejecutando Jobs","content":"    Slurm  Ciclo de Vida de un Job Batch Jobs Jobs Interactivos        Para enviar a ejecutar, cancelar y revisar el status de los jobs en Khipu, hacemos uso del gestor de recursos y scheduler de jobs Slurm.\nSlurm     Simple Linux Utility of Resource Mangement (Slurm), es el encargado de coordinar los recursos de todos los nodos del cluster y asignarlos de acuerdo la prioridad de los jobs, cantidad de recursos solicitados y cantidad de recursos disponibles. Slurm tiene un reparto de prioridad justo, donde cada job tiene una prioridad que depende de: a) los recursos usados por el usuario o grupo, b) la contribución del grupo al clúster y c) el tiempo en fila. Por favor, revisar grupos en Khipu para más detalles de recursos.\nCiclo de Vida de un Job     Enviar un job involucra especificar los recursos necesarios y la secuencia de comandos que deseamos ejecutar en los nodos de cómputo. Los pedidos se realizan a grupos de nodos de cómputo llamadas particiones. Las particiones, posen sus límites y propósitos de uso. En Khipu tenemos particiones especificas para cada grupo de usuarios. Una vez enviados, los jobs esperan en una cola y están sujetos a factores extra como sus prioridades de schedulling. Cuando el proceso de schedulling de un job inicia, los comandos o aplicaciones especificados son ejecutados en los nodos de computo que el scheduller asigna para satisfacer las necesidades de recurso solicitadas. El resultado de la ejecución será impreso en pantalla o escrito en un archivo dependiendo de la forma como se envío el job.\nBatch Jobs     Son aquellos jobs que se envían a través de un bash script y que son ejecutada de manera no-interactiva (no existe interacción con los I/O). Los scripts de envió se conforman de tres partes:\n Un primera linea hashbang la cual especifica el programa que va a ejecutar el script. Por lo general es #!/bin/bash. Una lista de directivas con los requisitos del job. Estas lineas tienen que estar antes de cualquier comando o definición de variables de ambiente, caso contrario serán ignoradas. Los comandos o aplicaciones que se van ejecutar en el job.  A continuación se muestra un ejemplo básico de un script.\n#!/bin/bash  ## Directivas # Job name: #SBATCH --job-name=simple_example  # Nombre del archivo de salida: #SBATCH --out=\u0026#34;slurm-%j.out\u0026#34; # Tiempo limite de ejecución: #SBATCH --time=01:00 # Cantidad de recursos #SBATCH --nodes=1 --ntasks=1 --cpus-per-task=2  ## Secuencia de comandos echo \u0026#34;Iniciando en $(date)\u0026#34; echo \u0026#34;El job fue enviado a la partición ${SLURM_JOB_PARTITION}, la partición por defecto es ${SLURM_CLUSTER_NAME}\u0026#34; echo \u0026#34;Nombre del job: ${SLURM_JOB_NAME}, Job ID: ${SLURM_JOB_ID}\u0026#34; echo \u0026#34;Tengo ${SLURM_CPUS_ON_NODE}CPUs en el nodo $(hostname)\u0026#34; echo \u0026#34;Voy a dormir 10s para que me veas en la fila\u0026#34; sleep 10 Para probarlo, deberemos guardarlo con el siguiente formato ej-simple.sh y lo mandaremos ejecutando.\nsbatch ej-simple.sh Ejemplos más detallados de bash scripts se muestran aquí.\nJobs Interactivos     Los jobs interactivos son aquellos que pueden ser usados para testear y encontrar errores en un código. Al pedir un job interactivo, se reservaran los recursos y se iniciará sesión en un shell de alguno de los nodos de cómputo. A continuación se muestra un ejemplo de job interactivo:\nsrun --pty -t 2:00:00 --mem=8G -p docencia bash El comando anterior asignará un CPU y 8GiB RAM por un periodo de 2 horas. Durante ese tiempo podremos ejecutar comandos dentro de la shell de manera interactiva. Para salir de la sesión, deberemos ejecutar `exit` o presionar `Ctrl`+`d`. "},{"id":19,"href":"/guia_de_usuario/ejecutando_trabajos/recursos_a_usar/","title":"¿Qué recursos pueden usarse?","parent":"Ejecutando Jobs","content":"https://docs.ycrc.yale.edu/clusters-at-yale/job-scheduling/resource-requests/\n"},{"id":20,"href":"/guia_de_usuario/ejecutando_trabajos/ejecutar_y_revisar_jobs/","title":"Ejecutar y Revisar Jobs","parent":"Ejecutando Jobs","content":"    Comandos Básicos de Slurm Opciones de Job Request más comunes      Comandos Básicos de Slurm     A continuación se muestra una lista con los comandos de Slurm más usados:\n Para enviar un batch job a la cola de slurm ejecutaremos sbatch.  sbatch ejemplo.sh  Para listar los jobs que he enviado a la cola usaremos squeue.  squeue --me  Para cancelar un job en ejecución usando su job ID emplearemos scancel.  scancel 78910  Para revisar el estatus de un job usaremos sacct y le pasaremos un job ID.  sacct -j 78910  Para revisar cuan eficientemente un job se ejecuta, emplearemos seff acompañado del job ID.  seff 78910  Para ejecutar un job de manera interactiva emplearemos srun  srun --pty -t 2:00:00 --mem=8G -p interactive bash Opciones de Job Request más comunes     Las siguientes opciones modifican el tamaño, largo y el comportamiento del job qye se envía. Estos pueden especificarse llamando a srun o sbatch, o dentro de un batch job. Si se especifican las opciones en los argumentos de sbatch y en el script del batch job al mismo tiempo, las opciones pasadas al comando sbatch serán las que se tomarán en cuenta. Si no se especifica valor para alguna de las opciones, los valores por defecto serán los que se empleen.\n .myTable { border-radius: 5px; } .myTable th { background-color:var(--body-font-color); color: white; }     Opción Larga Opción Corta Valor por Defecto Descripción     --job-name -J Nombre del archivo Nombre de job personalizado.   --output -o \u0026quot;slurm-%j.out\u0026quot; Nombre del archivo donde se guadará la salida stdout o stderr. Mayores patrones de nombre aquí.   --error -e Se escribe en el mismo archivo del --output Nombre del archivo donde se guadarán los logs de ;ps errores.   --partition -p Varía de acuerdo al cluster Señala la partición donde se va a ejecutar el job.   --account -A El nombre de su grupo Especifica si se tiene acceso a múltiples particiones privadas.   --time -t Varía de acuerda a la partición Límite de tiempo para el job en el formato D-HH:MM:SS. Por ejemplo, -t 1- es un día de ejecución y -t 4:00:00 son 4 horas.   --nodes -N 1 Número total de nodos.   --ntasks -n 1 Número de tareas (workers MPI).   --ntasks-per-node  El scheduler lo decide Número de tareas por nodo.   --cpus-per-task -c 1 Número de CPUs para cada tarea. Use esto para threads/cores en un job de nodo único.   --mem-per-cpu  5G Cantidad de memoria RAM requerida por CPU en MiB. Si se especifica en GiB usar G(ej. 10GB).   --mem   Memoria pedida por nodo en MiB. Si se especifica en GiB usar G(ej. 10GB).   --gpus G  Usado para pedir GPUs.   --constraint C  Restricciones a las características del nodo. Para limitar los tipos de nodos a ejecutarse.   --mail-user  Tu email de UTEC Dirección de correo a donde enviar notificaciones del job.   --mail-type  Ninguna Envía un mail cada vez que un job cambia de estado. Utilice la opción ALL para recibir notificaciones al iniciar y terminar un job. Opciones disponibles ALL, BEGIN, END, FAIL, NONE    "},{"id":21,"href":"/guia_de_usuario/ejecutando_trabajos/ejemplos_de_jobs/","title":"Ejemplos de Jobs Scripts","parent":"Ejecutando Jobs","content":"    Ejemplos Introductorios  Imprimir la fecha actual Imprimir las variables de ambiente de Slurm   Ejemplos con OpenMP, MPI e Híbridos  OpenMP de un solo thread OpenMP de múltiples threads MPI multi-proceso MPI Y OpenMP a la vez (Híbrido)   Ejemplos Diversos  Ejecución de un programa en Python   Ejemplos de Job Scripts Avanzados  Jobs dependientes Pasar variables de ambiente a jobs        Ejemplos Introductorios     Imprimir la fecha actual      Crear el archivo fecha_actual.sh:  #!/bin/bash  # Nombre del job: #SBATCH --job-name=fecha_actual  # Cantidad de CPUs cores a usar: #SBATCH -c 1 # Tamaño de memoria del job: #SBATCH --mem-per-cpu=100mb date sleep 10 # duerme 10s, solo para visualizar el job en la fila Enviar a ejecutar el job:  sbatch fecha_actual.sh Imprimir las variables de ambiente de Slurm      Crear el archivo env_vars.sh:  #!/bin/bash  # Nombre del job: #SBATCH --job-name=env_vars  # Comandos: set | grep SLURM Enviar a ejecutar el job:  sbatch env_vars.sh Ejemplos con OpenMP, MPI e Híbridos     Para los siguientes dos ejemplos con OpenMP usaremos el siguiente código en C++ prueba_openmp.cpp.\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;omp.h\u0026gt; void Hello(void); /* Thread function */ /*--------------------------------------------------------------------*/ int main(int argc, char* argv[]) { int thread_count = strtol(argv[1], NULL, 10); # pragma omp parallel num_threads(thread_count)  Hello(); return 0; } /* main */ /*------------------------------------------------------------------- * * Function: Hello * * Purpose: Thread function that prints message * */ void Hello(void) { int my_rank = omp_get_thread_num(); int thread_count = omp_get_num_threads(); printf(\u0026#34;Hello from thread %d of %d\\n\u0026#34;, my_rank, thread_count); /* Hello */ } Compilaremos el programa antes de crear y enviar el batch script.\nmodule load gcc/5.5.0 g++ prueba_openmp.cpp -fopenmp -lpthread -o prueba_openmp module unload gcc/5.5.0 OpenMP de un solo thread      Crear el archivo single_thread_openmp.sh:  #!/bin/bash  # Nombre del job: #SBATCH --job-name=single_thread_openmp # Límite de tiempo de 10 min: #SBATCH --time=10:00 ./prueba_openmp Enviar a ejecutar el job:  sbatch single_thread_openmp.sh OpenMP de múltiples threads      Crear el archivo multi_thread_openmp.sh:  #!/bin/bash  # Nombre del job: #SBATCH --job-name=multi_thread_omp_job # Nombre del archivo de salida: #SBATCH --output=multi_thread_omp_job.txt # Numero de tasks: #SBATCH --ntasks=1 # Numero de CPUs por task: #SBATCH --cpus-per-task=4 # Límite de tiempo de 10 min: #SBATCH --time=10:00 export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK ./prueba_openmp Enviar a ejecutar el job:  sbatch multi_thread_openmp.sh MPI multi-proceso     Para el siguiente ejemplo usaremos el código prueba_mpi.c:\n#include \u0026lt;mpi.h\u0026gt;#include \u0026lt;stdio.h\u0026gt; int main(int argc, char** argv) { // Initialize the MPI environment MPI_Init(NULL, NULL); // Get the number of processes int world_size; MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;world_size); // Get the rank of the process int world_rank; MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;world_rank); // Get the name of the processor char processor_name[MPI_MAX_PROCESSOR_NAME]; int name_len; MPI_Get_processor_name(processor_name, \u0026amp;name_len); // Print off a hello world message printf(\u0026#34;Hello world from processor %s, rank %d out of %d processors\\n\u0026#34;, processor_name, world_rank, world_size); // Finalize the MPI environment. MPI_Finalize(); }  Compilar el programa mpi:  module load mpich/4.0 mpicc prueba_mpi.c -o prueba_mpi module unload mpich/4.0 Crear el archivo multi_process_mpi.sh:  #!/bin/bash  # Nombre del job: #SBATCH -J prueba_mpi # Nombre de la partición: #SBATCH -p investigacion # Número de nodos: #SBATCH -N 2  # Número de tasks por nodo: #SBATCH --tasks-per-node=3  ## Carga del modulo MPICH 4.0 module load mpich/4.0 # Ejecución del compilado mpirun prueba_mpi ## Descarga del módulo module unload mpich/4.0 Enviar a ejecutar el job:  sbatch multi_process_mpi.sh MPI Y OpenMP a la vez (Híbrido)     Para este ejemplo usaremos el siguiente codigo hibrido y lo guardaremos en un archivo hibrido.c.\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;omp.h\u0026gt;#include \u0026#34;mpi.h\u0026#34; int main(int argc, char *argv[]) { int numprocs, rank, namelen; char processor_name[MPI_MAX_PROCESSOR_NAME]; int iam = 0, np = 1; MPI_Init(\u0026amp;argc, \u0026amp;argv); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;numprocs); MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); MPI_Get_processor_name(processor_name, \u0026amp;namelen); #pragma omp parallel default(shared) private(iam, np)  { np = omp_get_num_threads(); iam = omp_get_thread_num(); printf(\u0026#34;Hello from thread %d out of %d from process %d out of %d on %s\\n\u0026#34;, iam, np, rank, numprocs, processor_name); } MPI_Finalize(); }  Compilar el programa híbrido:  module load mpich/4.0 mpicc -fopenmp hibrido.c -o hibrido module unload mpich/4.0 Crear el archivo hibrido_mpi_openmp.sh:  #!/bin/bash  # Un Job script para la ejecución de un código híbrido MPI-OpenMP #SBATCH --job-name=hibrido_mpi_openmp #SBATCH --output=hibrido_mpi_openmp.out #SBATCH --ntasks=4 #SBATCH --cpus-per-task=8 #SBATCH --partition=docencia # Cargar el modulo MPI. module load mpich/4.0 # Configurar el valor de OMP_NUM_THREADS con el numero de CPUs por task solicitado. export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK # Ejecutar el proceso con mpirun. Puede notar que no es necesario especificar el flag de MPI # \u0026#39;-n\u0026#39; puesto que automaticamente utiliza el valor de la configuración de Slurm realizada mpirun ./hibrido module unload mpich/4.0 Enviar a ejecutar el job:  sbatch hibrido_mpi_openmp.sh Ejemplos Diversos     Ejecución de un programa en Python     Ejecución del programa en Python, prueba_python.py:\nfrom math import factorial as f print(\u0026#34;Hola mundo\u0026#34;) N = 100 print(\u0026#34;%d! = %d\u0026#34; %(N, f(N)))  Crear el archivo ej5.sh:  #!/bin/bash #SBATCH -J ej5 # nombre del job #SBATCH -p investigacion # nombre de la particion  #SBATCH -c 1 # numero de cpu cores a usar module load python/2.7.17 # carga el modulo de python version 2.7.17 python2.7 prueba_python.py # siendo prueba_python.py el nombre del programa python module unload python/2.7.17 Enviar a ejecutar el job:  sbatch ej5.sh Ejemplos de Job Scripts Avanzados     Jobs dependientes     Ejecución de dos jobs, donde uno depende del término del otro.\n Crear los archivos ej7-1.sh y ej7-2.sh:  #!/bin/bash #SBATCH -J ej7-1 #SBATCH -N 1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB echo \u0026#34;ej7-1\u0026#34; date sleep 120 date #!/bin/bash #SBATCH -J ej7-2 #SBATCH -N 1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB echo \u0026#34;ej7-2\u0026#34; date sleep 120 date Enviar a ejecutar los jobs:  n_proc=$(sbatch ej7-1.sh) sbatch -d after:${n_proc##* } ej7-2.sh squeue Pasar variables de ambiente a jobs     Muestra como pasar variables a los jobs, esto permite setear variables de ambiente para la ejecución dentro de nuestro script. No es recomendable usar seteos de variables direcamente como comandos Unix. Existen dos formas:\n Colocar el set de variables que se necesita en el script mediante el flag --export=var1=valor,var2=valor….   Crear el archivo ej8-1.sh:  #!/bin/bash #SBATCH -J ej8-1 #SBATCH -N 1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB #SBATCH --export=var1=12 echo $var1  Enviar a ejecutar el job:  sbatch ej8-1.sh Especificar el valor de las variables al momento del envío del script. Brinda flexibilidad y es recomendable usarlo cuando solo varían pocos parámetros, como el nombre de un archivo o valor único.   Crear el archivo ej8-2.sh:  #!/bin/bash #SBATCH -J ej8-2 #SBATCH -N 1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB echo $var1  Enviar a ejecutar el job:  sbatch --export=var1=15 ej8-2.sh ## Información adicional - [Tutorial de Linux](https://www.tecmint.com/free-online-linux-learning-guide-for-beginners/) - [Documentación de Slurm](https://slurm.schedmd.com/quickstart.html)"},{"id":22,"href":"/guia_de_usuario/ejecutando_trabajos/monitorear_jobs/","title":"Monitorear Jobs","parent":"Ejecutando Jobs","content":"Jobs en ejecución     Si su job se enecuentra en ejecucion\n"},{"id":23,"href":"/guia_de_usuario/ejecutando_trabajos/scheduler_ejemplos/","title":"Ejemplos de Scheduler","parent":"Ejecutando Jobs","content":""},{"id":24,"href":"/guia_de_usuario/ejecutando_trabajos/scheduler_configuracion/","title":"Configuración de Scheduler","parent":"Ejecutando Jobs","content":""},{"id":25,"href":"/guia_de_usuario/transfiriendo_data/","title":"Transfiriendo Data","parent":"Guía de Usuario","content":""},{"id":26,"href":"/guia_de_usuario/configuracion_de_hardware/","title":"Configuracion de Hardware","parent":"Guía de Usuario","content":""},{"id":27,"href":"/ayuda/","title":"Ayuda","parent":"Inicio","content":"    Solicitando Ayuda     ¿Cómo pedir ayuda?     Preguntas que podemos atender     Recomendaciones       FAQs     Tutoriales     "},{"id":28,"href":"/ayuda/solicitando_ayuda/","title":"Solicitando Ayuda","parent":"Ayuda","content":""},{"id":29,"href":"/ayuda/solicitando_ayuda/como_pedir_ayuda/","title":"¿Cómo pedir ayuda?","parent":"Solicitando Ayuda","content":""},{"id":30,"href":"/ayuda/solicitando_ayuda/preguntas_a_atender/","title":"Preguntas que podemos atender","parent":"Solicitando Ayuda","content":""},{"id":31,"href":"/ayuda/solicitando_ayuda/recomendaciones/","title":"Recomendaciones","parent":"Solicitando Ayuda","content":""},{"id":32,"href":"/ayuda/faqs/","title":"FAQs","parent":"Ayuda","content":""},{"id":33,"href":"/ayuda/tutoriales/","title":"Tutoriales","parent":"Ayuda","content":""},{"id":34,"href":"/reportes/","title":"Reportes","parent":"Inicio","content":"Importante: Información actualizada cada 2 minutos.  Reporte General     Reporte de CPU      Reporte de Memoria     Reporte de Red       "},{"id":35,"href":"/anuncios/bienvenida/","title":"Bienvenida","parent":"Anuncios","content":"Lorem Ipsum \u0026ldquo;Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit\u0026hellip;\u0026rdquo; \u0026ldquo;There is no one who loves pain itself, who seeks after it and wants to have it, simply because it is pain\u0026hellip;\u0026rdquo;\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris pretium ligula eget metus sodales, id tincidunt tortor sollicitudin. Donec condimentum enim neque, placerat ornare est pretium at. Nulla ultrices dictum sem, at tempus nisl. Duis vitae euismod eros, eu scelerisque ante. Vestibulum dignissim leo a ipsum viverra, ut suscipit ex tempor. Cras consequat euismod nibh, ac molestie ligula varius et. Morbi a nisi at est ullamcorper mattis. Etiam iaculis felis quis nisi pellentesque, vitae iaculis nisi maximus. Phasellus venenatis non tellus vitae convallis. Donec quis cursus erat. Donec quis velit sed eros semper eleifend eget vel purus. Nullam scelerisque enim ac dapibus condimentum. Curabitur nec risus est.\nDonec at vestibulum felis. Mauris et nisl ut erat volutpat aliquet. Morbi semper risus mi, quis egestas orci sodales non. Nam rutrum commodo lectus ac maximus. Cras convallis lacus eget nulla sodales convallis. In auctor varius ex. Morbi eget malesuada magna. Aenean non pharetra velit, et blandit magna. Cras molestie sapien sed mauris rutrum molestie. Donec orci arcu, tempus a lorem non, feugiat pellentesque erat. Praesent eros nunc, dapibus id facilisis vitae, auctor ut quam. Sed justo est, dapibus ac enim ut, varius porttitor orci. Suspendisse vitae dolor vehicula, vestibulum lectus ac, dapibus odio.\nDonec ut velit id est bibendum posuere. Suspendisse cursus tempor tincidunt. Vestibulum consectetur orci eleifend, aliquam eros vitae, pulvinar nibh. Integer quis dictum quam, ut vehicula risus. Etiam rutrum placerat aliquet. Phasellus cursus tellus eu mi scelerisque, non suscipit lectus aliquet. Suspendisse elementum laoreet fringilla. Ut molestie vitae odio at accumsan. Cras vel eros quam. Donec ex metus, faucibus nec volutpat non, commodo ut turpis.\nQuisque ultricies efficitur tortor, eu viverra arcu consectetur varius. Nunc sed molestie diam. Aliquam a nisl sed magna mollis volutpat eget in dui. Donec efficitur odio a libero malesuada tincidunt. Duis vitae odio non quam pulvinar porta. In et imperdiet augue. Sed ornare eleifend nisi. Quisque aliquam consequat metus nec luctus.\nIn ut risus a nibh laoreet elementum. Nam sed nulla ipsum. Nunc pharetra vestibulum erat ac tempor. Quisque arcu neque, ultricies eget elementum sit amet, tempor faucibus turpis. Suspendisse lobortis rutrum pharetra. Nulla mollis facilisis lectus, eu luctus quam congue in. Sed congue facilisis sem at efficitur. Cras nec urna ligula. Interdum et malesuada fames ac ante ipsum primis in faucibus. Nam nibh ante, porttitor vel fermentum sed, fermentum eget elit. Generated 5 paragraphs, 388 words, 2582 bytes of Lorem Ipsum\n"},{"id":36,"href":"/","title":"Inicio","parent":"","content":"Bienvenido a la documentación de Khipu   Esta es la documentación oficial del cluster Khipu. En este sitio encontrarás toda la información sobre el cluster, las políticas y guías de uso.\nIniciar   Enlaces Rápidos   Primeros Pasos   Si no sabe por donde empezar con la documentación, este es un buen punto de comienzo.  Guía de Usuario   Si desea información detallada sobre el uso y manejo del cluster, puede consultar aquí.  Reportes   Información actualizada cada dos minutos sobre el estado actual del cluster está aquí.   Anuncios   La información actualizada más relevante correspondiente al funcionamiento del cluster está aquí.   Zero initial configuration   Getting started in minutes. The theme is shipped with a default configuration and works out of the box.  Handy shortcodes   We included some (hopefully) useful custom shortcodes so you don\u0026rsquo;t have to and can focus on writing amazing docs.  Dark mode   Powerful dark mode that detects your system preferences or can be controlled by a toggle switch.   Sobre Khipu   Khipu es un cluster dedicado a la computación de alto desempeño, en inglés High performance Computing (HPC). Está formado por una colección de servidores distribuídos, llamados nodos que se encuentran conectados a través de una interconexión de alta velocidad (Infiniband).\nKhipu es parte del Centro de Investigación en Computación Sostenible (COMPSUST) de la Universidad de Ingeniería y Tecnología (UTEC).\n Contacto: khipu@utec.edu.pe.\nPara información de acceso revisar aquí.\n Grupos de usuarios   Existen dos grupos de usuarios gerenciados automáticamente por Slurm: 1) investigación, y 2) educación. Ambos grupos acceden al cluster por el nodo líder para enviar trabajos a la fila de ejecución.\nGrupo Educación   Permite el uso del cluster para aula o laboratorio por los estudiantes a pedido de un instructor(a) registrado(a). Este grupo es financiado por la universidad y sus trabajos serán procesados bajo las siguientes características:\n Ocupar un nodo CPU y/o GPU. Equivalente a un total de 72 cores, 320 GB de memoria y 1.4 TB de almacenamiento.\n Tiempo máximo de ejecución de trabajo: 2 horas.\n Número de procesos concurrentes: 2.\n Cantidad de cores por proceso 8.  Grupo Investigación   Este grupo está financiado por UTEC, proyectos de investigación (PI), Departamentos y Dirección de Escuela. Los fondos centrales cubren costos de infraestructura, operación y soporte. Los PI y algunas unidades y departamentos financian la adquisición de nuevos nodos de procesamiento y almacenamiento. Los usuarios de este grupo ejecutan trabajos en todos los nodos bajo las siguientes características:\n Ocupar cualquier nodo CPU y/o GPU. Equivalente a un total de 200 cores, 1.1 TB de memoria y 5.3 TB de almacenamiento.\n Tiempo máximo de ejecución de trabajo NaN.\n Número de procesos concurrentes NaN.\n Cantidad de cores por proceso NaN.   Infraestructura   Para mayor información sobre programas, ver Software\n .myTable { border-radius: 5px; } .myTable th { background-color:var(--header-background); color: white; }     nLíder     Nodo de acceso al cluster, usado principalmente para compilar y enviar trabajos.   Procesadores: Intel(R) Xeon(R) Gold 6230 CPU @ 2.10 GHz 20 cores por socket, 40 por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 480 SSD, total 40 TB HDD   Network: Infiniband FDR MT4119       nCPU (1, 2, 3, 4, 5)     Usado para procesamiento, gerenciado automáticamente por Slurm.   Procesadores: Intel(R) Xeon(R) Gold 6130 CPU @2.10 GHz 16 cores por socket, 32 por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 960 GB SSD   Network: Infiniband FDR MT4119       nGPU 1     Usado para procesamiento, gerenciado automáticamente por Slurm.   Procesadores: Intel(R) Xeon(R) Gold 6230 CPU @2.10 GHz 20 cores por socket, 40 por nodo.   Gráficos: NVIDIA Tesla T4 16 GB GDDR6, PCIe 3.0 x16 1 GPU por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 480 GB SSD   Network: Infiniband FDR MT4119       nGPU 2     Usado para procesamiento, gerenciado automáticamente por Slurm.   Procesadores: AMD EPYC 7742 @2.24 GHz 64 cores por socket, 128 por nodo.   Gráficos: NVIDIA Tesla T4 16 GB GDDR6, PCIe 3.0 x16 1 GPU por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 480 GB SSD   Network: Infiniband FDR MT4119     --   "},{"id":37,"href":"/tags/","title":"Tags","parent":"Inicio","content":""}]