[{"id":0,"href":"/overview/","title":"Información","parent":"Inicio","content":"    Primeros Pasos      Sobre Khipu      Infraestructura      Galería      "},{"id":1,"href":"/cuentas/","title":"Cuentas","parent":"Inicio","content":"    Grupos de Cuentas      Solicitud de Acceso      Políticas      "},{"id":2,"href":"/overview/primeros_pasos/","title":"Primeros Pasos","parent":"Información","content":"    Clusters HPC Khipu Solicitar una cuenta Iniciar Sesión Enviar un Job Linux Enviar Archivos Uso de Software Reglas de uso Obtener ayuda      Clusters HPC Khipu     Visto de manera sencilla, Khipu es una colección de computadoras que que se encuentran conectadas entre sí y que trabajan como si fueran una sola. A cada computador conectado se le conoce como nodo. El nodo de acceso, o también llamado nodo líder, es aquel al cual nos conectamos de manera remota para acceder al cluster. Los nodos que se encargan de la ejecución de nuestros jobs o trabajos, son llamados nodos de cómputo. Para enviar un job hacemos uso de un scheduler, el cual se encargado de manejar los diferentes jobs que se envían y garantizar equidad al momento que pasan a ejecutarse. Todos los nodos de computo, poseen un filesystem compartido lo cual permite a los jobs acceder y modificar la data de cualquier nodo sin la necesidad de copiarlo al otro.\nSolicitar una cuenta     Para poder acceder al cluster Khipu es necesario solicitar una cuenta. Las cuentas se dividen en dos grupos: educación e investigación. Las solicitudes de cuentas no implican costo alguno para los miembros de UTEC y pueden ser solicitadas por el docente del curso o el encargado del proyecto de investigación. Mayores detalles sobre las cuentas en Khipu lo encuentra aquí.\nAbrir solicitud de acceso    Una vez completado el formulario y aprobada la solicitud de acceso, las credenciales serán enviadas a los correos registrados. A partir del momento en que se reciben las credenciales, el usuario se compromete a cumplir las políticas de uso.\nIniciar Sesión     Una vez adquirida una cuenta, puede acceder al cluster por terminal via SSH. Diríjase al apartado de inicio de sesión para conocer cómo realizar este proceso.\nEnviar un Job     Dentro del cluster, usted puede enviar a ejecución sus trabajos o jobs a través de un scheduler. Una vez adquirida una cuenta, puede acceder al cluster por terminal via SSH. Diríjase al apartado de enviar jobs para conocer cómo realizar este proceso.\nLinux     Enviar Archivos     Uso de Software     Reglas de uso     Obtener ayuda     "},{"id":3,"href":"/anuncios/","title":"Anuncios","parent":"Inicio","content":""},{"id":4,"href":"/cuentas/cuentas_del_cluster/","title":"Grupos de Cuentas","parent":"Cuentas","content":"Existen dos grupos de usuarios gerenciados automáticamente por Slurm: 1) educación, y 2) investigación. Ambos grupos acceden al cluster por el nodo líder para enviar trabajos a la fila de ejecución.\nGrupo Educación     Permite el uso del cluster para aula o laboratorio por los estudiantes a pedido de un instructor(a) registrado(a). Este grupo es financiado por la universidad y sus trabajos serán procesados bajo las siguientes características:\n Ocupar un nodo CPU y/o GPU. Equivalente a un total de 72 cores, 320 GB de memoria y 1.4 TB de almacenamiento.\n Tiempo máximo de ejecución de trabajo: 2 horas.\n Número de procesos concurrentes: 2.\n Cantidad de cores por proceso 8.  Grupo Investigación     Este grupo está financiado por UTEC, proyectos de investigación (PI), Departamentos y Dirección de Escuela. Los fondos centrales cubren costos de infraestructura, operación y soporte. Los PI y algunas unidades y departamentos financian la adquisición de nuevos nodos de procesamiento y almacenamiento. Los usuarios de este grupo ejecutan trabajos en todos los nodos bajo las siguientes características:\n Ocupar cualquier nodo CPU y/o GPU. Equivalente a un total de 200 cores, 1.1 TB de memoria y 5.3 TB de almacenamiento.\n Tiempo máximo de ejecución de trabajo NaN.\n Número de procesos concurrentes NaN.\n Cantidad de cores por proceso NaN.  "},{"id":5,"href":"/cuentas/solicitud_de_acceso/","title":"Solicitud de Acceso","parent":"Cuentas","content":"Para poder obtener credenciales de acceso a Khipu, es necesario completar un formulario con información relevante del proyecto o curso. Este formulario deberá ser completado por: a) el investigador principal del proyecto (PI), o b) el profesor responsable del curso.\nEl formulario es ligeramente distinto para cada grupo de cuentas. Sin embargo, en ambos casos se deberá completar el nombre del proyecto o curso, la fecha de inicio y fin, el investigador o docente a cargo, los miembros del curso o proyecto y la lista de software licenciado o de código abierto a emplearse. Esta última información nos es útil para poder identificar los softwares con mayor demanda y así crear los módulos correspondientes.\nPuede acceder a la solicitud acceso a través del siguiente enlace:\nAbrir solicitud de acceso    También se puede acceder al formulario haciendo click a la opción Registro dentro de https://web.khipu.utec.edu.pe/\n Una vez completado el formulario, se obtendrá respuesta a la solicitud en un plazo no mayor de 2 días hábiles. Si la solicitud es favorable, también se realizará el envío de las credenciales de acceso a los correos electrónicos de cada miembro registrado. Una vez que un usuario recibe sus credenciales se compromete a cumplir con las políticas de uso. Por otro lado, si la solicitud no es favorable, se enviará por correo las razones de la decisión.\n"},{"id":6,"href":"/cuentas/politica/","title":"Políticas","parent":"Cuentas","content":"    Sobre las cuentas Sobre el uso Incumplimiento de las reglas de uso Agradecimientos por el uso de Khipu Comunicación      El uso del cluster HPC Khipu esta sujeto al cumplimiento de sus políticas de uso. Una vez que se reciben las credenciales de acceso, los usuarios aceptan cumplir las siguiente políticas.\nSobre las cuentas      Solo los usuarios registrados pueden acceder a Khipu. Asimismo, los usuarios se hacen responsables de tomar las precauciones necesarias para proteger sus credenciales de acceso y así impedir accesos no autorizados. Los usuarios se encuentran prohibidos de compartir sus credenciales con otras personas, así sean estos estudiantes o colaboradores. Del mismo modo, los usuarios se encuentran prohibidos de acceder a Khipu con credenciales que no son suyas, con o sin consentimiento del usuario. En caso de existir sospecha que otros han usado su cuenta, notificarlo inmediatamente a khipu@utec.edu.pe. Las cuentas serán desactivadas si una de las siguientes condiciones se cumple:  La fecha de fin del curso o proyecto ha sido alcanzada, o El encargado del curso o proyecto indica que el usuario ya no requiere acceso, o El usuario ha recibido alguna sanción disciplinaria grave de parte de la universidad.    Sobre el uso     El cluster Khipu es un recurso compartido por multiples usuarios, así que las acciones que usted realice pueden afectar a otros usuarios si no se realizan de manera adecuada. Es por ello, que se brindan las siguientes recomendaciones a fin de garantizar un uso equitativo y justo de los recursos del cluster.\n El uso del cluster se encuentra restringido solo para fines educativos y de investigación. Su uso no debe estar relacionado a actividades comerciales, de consultoría o creación de software malicioso. Los usuarios son responsables de usar los recursos del cluster de manera eficiente, efectiva, ética y lícita.   El nodo de administración es usado para acceder al cluster, editar archivos, compilar código y enviar trabajos. De ninguna manera, se deben ejecutar los trabajos directamente en el nodo de administración. Toda ejecución de trabajos debe ser realizada a traves del gestor de recursos slurm. Es así como se garantizan un uso eficiente y justo de los recursos. El cluster no debe ser utilizado como almacenamiento personal. Se recomienda copiar sus resultados a su máquina personal una vez culminado el uso del cluster. Los usuarios no deben intentar acceder a cualquier archivo o programa al cual no poseen autorización o un consentimiento explicito del dueño del archivo o programa. Los usuarios pueden hacer uso de las aplicaciones ya instaladas, y también compilar nuevas aplicaciones en su espacio personal solamente si fueran necesarios para el desarrollo de su proyecto o trabajo. Los softwares que se instalen deben incluir una licencia válida (si es aplicable). No se puede instalar software de procedencia ilegal y/o con licencia no otorgada por los desarrolladores. La presente política será revisada y actualizada de manera periódica. Cualquier cambio será comunicado al correo electrónico registrado.  Incumplimiento de las reglas de uso     El usuario acepta cumplir con la normativa y sanciones impuestas por UTEC en sus atribuciones.\n En caso de no cumplir normas de UTEC y/o atentar contra la ley, el evento será notificado a las autoridades pertinentes. En caso de incumplimiento de los items anteriores, la cuenta será suspendida.  Agradecimientos por el uso de Khipu     Alentamos a todos a los usuarios cuyo proyecto termina en una publicación o presentación a añadir al cluster Khipu dentro de los agradecimientos. Nos ayudaría de sobremanera, mencionar el rol que tuvo Khipu a lo largo de su investigación o formación académica. Su sola mención contribuye a comunicar el rol de Khipu en el desarrollo de investigaciones dentro de la universidad, incentiva a más estudiantes y docentes a incorporar a Khipu dentro de sus proyectos o planes de estudio, mantiene el financiamiento y soporte que hace posible que Khipu siga creciendo cada día más. Puede añadir un mensaje de agradecimiento como el siguiente:\n \u0026ldquo;El trabajo computacional del presente producto fue apoyado por los recursos del cluster HPC Khipu (https://web.khipu.utec.edu.pe/) de la Universidad de Ingeniería y Tecnología (UTEC) \u0026quot;\n Comunicación     El correo electrónico ingresado al momento de solicitar acceso a Khipu será el medio por el cual se enviarán anuncios relevantes al funcionamiento del cluster. Si usted presenta preguntas, pedidos o requiere asistencia puede escribir de manera directa a khipu@utec.edu.pe. Por favor, asegúrese de seguir las recomendaciones de soporte a fin de poder atender sus requerimientos de mejor manera.\n"},{"id":7,"href":"/cuentas/recomendaciones/","title":"Recomendaciones","parent":"Cuentas","content":""},{"id":8,"href":"/guia_de_usuario/","title":"Guía de Usuario","parent":"Inicio","content":"    Iniciar Sesión      Software     Software Disponible      Usando Software     Instalando Software        Ejecutando Jobs     Ejecutar y Revisar Jobs      ¿Qué recursos pueden usarse?     Enviar Jobs      Monitorear Jobs     Ejemplos de Scheduler     Configuración de Scheduler       Transfiriendo Data     Configuracion de Hardware     "},{"id":9,"href":"/overview/khipu/","title":"Sobre Khipu","parent":"Información","content":"    Sobre Khipu  ¿Qué es computación de alto desempeño?        Sobre Khipu     Khipu es un cluster de computación de alto desempeño que forma Centro de Investigación para la Computación Sostenible (COMPSUST) y el Departamento de Ciencia de la Computación (CS) de la Universidad de Ingeniería y Tecnología (UTEC).\n Misión\nSomos un grupo académico que apostamos por el desarrollo científico de nuestra comunidad, a traves de la computación de alto desempeño.\n   Visión\nSer un laboratorio referente en computación de alto desempeño a nivel local, y ser reconocidos por nuestra colaboración al desarrollo científico de nuestra comunidad.\n   ¿Qué es computación de alto desempeño?     La computación de alto desempeño, o HPC por sus siglas en inglés, es la práctica de agregar poder computacional de diferentes ordenadores, de tal manera que se obtiene un desempeño mucho mayor al de un ordenador convencional aprovechando el poder del paralelismo. Todo ello con el objetivo de resolver de manera más rápida los problemas complejos de campos como matemáticas, ciencias e ingenierías.\nHPC nos permite resolver problemas complejos que tardarían años, meses o semanas para resolverse en un computador convencional, pero que con el uso de HPC, se podría reducir a días, horas o minutos.\nLeer más:\n What is High Performance Computing?  What is High-Performance Computing (HPC)?  "},{"id":10,"href":"/guia_de_usuario/iniciar_sesion/","title":"Iniciar Sesión","parent":"Guía de Usuario","content":"    Inicio de sesión regular Inicio de sesión con SSH Keys  Linux o MacOS Windows        Inicio de sesión regular     Para poder iniciar sesión en Khipu deberemos seguir los siguientes pasos.\n Abrir un terminal y escribir el siguiente comando reemplazando su nombre de usuario en el campo .  ssh \u0026lt;username\u0026gt;@khipu.utec.edu.pe Ejemplo:\nssh juana.perez@khipu.utec.edu.pe Aparecerá el siguiente diálogo. Aceptar escribiendo yes y presionar enter. Luego colocar el password (este no aparecerá en pantalla al escribir) y presionar enter.  Are you sure you want to continue connecting (yes/no)? juana.perez@khipu.utec.edu.pe\u0026#39;s password: Al lograr accesar al cluster aparecerá el prompt del terminal remoto.  Last login: Fri Aug 21 11:28:13 2020 from 190.236.197.233 [juanaperez@khipu ~]$ Para cerrar o salir del cluster:  [juana.perez@khipu ~]$ exit Inicio de sesión con SSH Keys     Sobre las SSH Keys ↕  ¿Qué son las SSH Keys?     SSH o Secure Shell Keys son un conjunto de información que se emplea para identificar y encriptar la comunicación entre su máquina local y un servidor. Se compone de dos archivos: una clave pública (id_rsa.pub) y otra privada (id_rsa o id_rsa.ppk). Se manera simple la clave pública es un \u0026ldquo;candado\u0026rdquo; y la clave privada es la llave para abrir dicho \u0026ldquo;candado\u0026rdquo;. El \u0026ldquo;candado\u0026rdquo; (llave pública) puede ser compartida sin mayores problemas, en cambio si comparte su llave privada, alguien podría suplantar su identidad.\nCuando se conecta a un servidor, este presentará su llave pública. Usted provará su identidad mediante el uso de su llave privada. La comunicación con el servidor remoto y toda la información enviado durante este proceso será encriptada mediante su llave pública, de tal manera que solo usted pueda desencriptarla usando su llave privada.\n  Si desea acceder a Khipu sin escribir su contraseña deberá copiar la llave pública de su computador al cluster. La llave pública debe haber sido creada previamente con el siguiente comando.\nssh-keygen Su terminal responderá con:\nGenerating public/private rsa key pair. Enter file in which to save the key (/home/username/.ssh/id_rsa): Presione Enter para aceptar el valor por defecto. Su terminal responderá con:\nEnter passphrase (empty for no passphrase): Usted deberá eligir un passphrase seguro. Este valor le ayudará a prevenir el acceso a su cuenta en caso su clave privada es robada. Mientras escribe su passphrase este no aparecerá en pantalla. Su terminal responderá con:\nEnter same passphrase again: Ingrese el passphrase nuevamente. Una vez hecho esto se generará el par de llaves dentro de un directorio .ssh en su directorio /home. Si usted olvida su passphrase, no podrá recuperarlo. En cambio, usted deberá generar y copiar al servidor un nuevo par de SSH Keys.\nA continuación se muestra como copiar su clave pública a Khipu desde diferentes sistemas operativos:\nLinux o MacOS     ssh-copy-id -i ~/.ssh/id_rsa.pub juana.perez@khipu.utec.edu.pe Windows     type %HOMEPATH%\\.ssh\\id_rsa.pub | ssh juana.perez@khipu.utec.edu.pe \u0026#34;mkdir .ssh; cat \u0026gt;\u0026gt; .ssh/authorized_keys\u0026#34; "},{"id":11,"href":"/guia_de_usuario/software/","title":"Software","parent":"Guía de Usuario","content":""},{"id":12,"href":"/overview/infraestructura/","title":"Infraestructura","parent":"Información","content":"    Vista general Nodo de administración Nodos de computación  Nodos CPU Nodos GPU   Software del sistema Aplicaciones de software      Vista general      .myTable { border-radius: 5px; } .myTable th { background-color:var(--header-background); color: white; }  Nodo de administración     Nodo de acceso al cluster, usado principalmente para compilar y enviar trabajos.\n   Especificaciones     Nombre nLíder   Procesador Intel(R) Xeon(R) Gold 6230 CPU @ 2.10 GHz 20 cores por socket, 40 por nodo.    Memoria DRAM DDR4-1333 MHz, 128 GB por nodo    Almacenamiento 480 SSD, total 40 TB HDD    Red Infiniband FDR MT4119    Nodos de computación     Usado para procesamiento, gerenciado automáticamente por Slurm.\nNodos CPU        Especificaciones     Nombre nCPU   Procesador Intel(R) Xeon(R) Gold 6130 CPU @2.10 GHz 16 cores por socket, 32 por nodo.   Memoria DRAM DDR4-1333 MHz, 128 GB por nodo    Almacenamiento 960 GB SSD   Red Infiniband FDR MT4119    Nodos GPU        Especificaciones     Nombre nGPU   Procesador Intel(R) Xeon(R) Gold 6230 CPU @2.10 GHz 20 cores por socket, 40 por nodo.    Gráficos NVIDIA Tesla T4 16 GB GDDR6, PCIe 3.0 x16 1 GPU por nodo.   Memoria  DRAM DDR4-1333 MHz, 128 GB por nodo    Almacenamiento 480 GB SSD   Red Infiniband FDR MT4119     Software del sistema      Sistema Operativo: CentOS Linux 7 Message Passing Library: MPICH Compiladores: Intel, GCC Job Scheduler: SLURM Manejo de software: Módulos de ambiente  Aplicaciones de software     Revise aquí la lista de softwares, herramientas y utilidades disponible en Khipu.\n"},{"id":13,"href":"/guia_de_usuario/software/software_disponible/","title":"Software Disponible","parent":"Software","content":"El cluster Khipu usa CentOS 7 (Linux/GNU) como sistema operativo principal y Slurm como manager de filas. Para listar desde su terminal el software disponible para el usuario.\nmodule avail La siguiente tabla muestra los comandos para poder cargar y usar el software en su entorno.\n   Nombre Versión Comando de uso Categoría     cmake 3.16.5 module load cmake/3.16.5 compiler   gcc 5.5.0 module load gcc/5.5.0 compiler   gcc 6.5.0 module load gcc/6.5.0 compiler   gcc 7.5.0 module load gcc/7.5.0 compiler   gcc 8.4.0 module load gcc/8.4.0 compiler   gcc 9.2.0 module load gcc/9.2.0 compiler   hdf5 1.10.5 module load hdf5/1.10.5 lib   mpich 3.1.4 module load mpich/3.1.4 compiler   mpich 3.3.2 module load mpich/3.3.2 compiler   openmpi 2.1.6 module load openmpi/2.1.6 compiler   openmpi 3.1.5 module load openmpi/3.1.5 compiler   openmpi 4.0.3 module load openmpi/4.0.3 compiler   python 2.7.17 module load python/2.7.17 language   python 3.7.7 module load python/3.7.7 language   mpich 1.5 module load mpich/1.5 compiler   mpich 3.2.1 module load mpich/3.2.1 compiler   pmix 2.1 module load pmix/2.1 lib    Ejemplo\nPara cargar gcc versión 7.5.0, usar el comando de uso sugerido:\nmodule load gcc/7.5.0 Una vez terminado de el usar el software es recomendable hacer unload. Esto también se usa en scripts de jobs (ver ejemplos en la Guía de Uso).\nmodule unload gcc/7.5.0 "},{"id":14,"href":"/guia_de_usuario/software/usando_software/","title":"Usando Software","parent":"Software","content":""},{"id":15,"href":"/guia_de_usuario/software/instalando_software/","title":"Instalando Software","parent":"Software","content":"Para entrar en contacto con el administrador, por favor enviar un mensaje a khipu@utec.edu.pe.\n En caso de software licenciado, el usuario debe proveer la licencia y/o instalador del software al administrador para ser instalado. El usuario puede instalar el software y libraries necesarios en su /home, en caso de no requerir licencia o ser código abierto. El usuario puede compilar en el nodo líder antes de enviar un job a ejecutar. No enviar jobs de compilación a la fila del cluster. Siendo Khipu un cluster enfocado al procesamiento y HPC; no se instalarán bases de datos, containers o máquinas virtuales. Si el software o library requiere permisos especiales para instalación, solamente el profesor encargado del curso o un investigador de proyecto pueden solicitar la instalación bajo coordinación con el administrador. En caso de necesitar libraries adicionales de Python, no crear ambientes virtuales para instalar. Solamente el profesor encargado del curso o un investigador de proyecto pueden solicitar la instalación bajo coordinación con el administrador.   Por favor entrar en contacto en caso se requiera un software o library adicional que va a ser usado por múltiples investigadores o participantes de un curso que no ha sido especificado en el Formulario de Solicitud de Acceso.\n "},{"id":16,"href":"/guia_de_usuario/ejecutando_trabajos/","title":"Ejecutando Jobs","parent":"Guía de Usuario","content":""},{"id":17,"href":"/overview/galeria/","title":"Galería","parent":"Información","content":"     Vista frontal del cluster HPC Khipu        Nodos del cluster HPC Khipu        Racks disponibles del cluster HPC Khipu        Vista de todos los nodos del cluster HPC Khipu        Vista general del cluster HPC Khipu en UTEC    "},{"id":18,"href":"/guia_de_usuario/ejecutando_trabajos/ejecutar_y_revisar_jobs/","title":"Ejecutar y Revisar Jobs","parent":"Ejecutando Jobs","content":"Sobre Slurm     Slurm es un gestor de filas de trabajos (jobs) que permite organizar de forma eficiente el uso de recursos compartido del cluster. Slurm asigna recursos a los jobs de acuerdo con la prioridad del trabajo y la cantidad de recursos solicitados versus disponibles. Slurm tiene un reparto de prioridad justo, donde cada job tiene una prioridad que depende de: a) los recursos usados por el usuario o grupo, b) la contribución del grupo al clúster y c) el tiempo en fila. Por favor, revisar grupos en Khipu para más detalles de recursos. Los comandos más usados son:\n sbatch: permite adicionar jobs a la fila mediante un script. Link del manual de sbatch. srun: permite adicionar jobs en modo interactivo lo que permite probar comandos que se usarán en el script así como depurarlo. Link del manual de srun. scancel: termina la ejecución de un determinado job, independientemente si está o no en ejecución. Link del manual de scancel. squeue: permite visualizar el estado actual de la fila de jobs. Link del manual de squeue.  "},{"id":19,"href":"/guia_de_usuario/ejecutando_trabajos/recursos_a_usar/","title":"¿Qué recursos pueden usarse?","parent":"Ejecutando Jobs","content":""},{"id":20,"href":"/guia_de_usuario/ejecutando_trabajos/enviar_jobs/","title":"Enviar Jobs","parent":"Ejecutando Jobs","content":"      Directivas de jobs Ejemplos de job scripts Ejemplos de job scripts con MPI, OpenMP y Python Ejemplos de job scripts avanzados Información adicional         Para mandar a ejecutar un job usando el script de ejemplo ej1.sh.  Mostrar ej1.sh ↕  #!/bin/bash #SBATCH --job-name=ej1 # nombre del job #SBATCH -c 1 # numero de cpu cores a usar #SBATCH --mem-per-cpu=100mb # tamano de memoria del job en ejecucion. date sleep 10 # duerme 10s, solo para visualizar el job en la fila    [juana.perez@khipu ~]$ sbatch ej1.sh Submitted batch job 91 Para mostrar la fila actual de jobs:  [juana.perez@khipu ~]$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 91 investigacion EJ2 juana.perez R 0:29 1 n001 Para mostrar todos los detalles del job 91:  [juana.perez@khipu ~]$ scontrol show job 91 Para cancelar la ejecución del job 91:  [juana.perez@khipu ~]$ scancel 91 Directivas de jobs     Importante: verificar los recursos de Khipu. Slurm regulará automáticamente en caso el usuario exceda las capacidades del Grupo de Usuario.     Directiva Versión corta Descripción     --mail-user=\u0026lt;email_address\u0026gt;  Correo electrónico   after:job_id[[+time][:jobid[+time]...]]  El job se mandará a la fila luego que otro job termine su ejecución.   --mail-type=\u0026lt;ALL,BEGIN,END,FAIL,NONE\u0026gt;  Enviar notificaciones de job a email   --gres=gpu:\u0026lt;cantidadGPU\u0026gt;  Especifica la cantidad de tarjetas GPU que se necesitan.   --export=var1=valor[,var2=valor...]  Especifica los valores de las variables de entorno en la ejecución del script.   --nodes=\u0026lt;nodes\u0026gt;  -N \u0026lt;nodes\u0026gt; Nodos requeridos por job.   --job-name=\u0026lt;Nombrejob \u0026gt;  -J \u0026lt;Nombrejob\u0026gt;  Nombre del job.   --cpus-per-task=\u0026lt;cpus\u0026gt;  -c \u0026lt;cpus\u0026gt; Núcleos CPU por tarea.   --nodes=\u0026lt;numeroNodos\u0026gt;  -N \u0026lt;numeroNodos\u0026gt; Número de nodos solicitados para la ejecución de los jobs.   --output= \u0026lt;rutaArchivo.log \u0026gt;  -o \u0026lt;rutaArchivo.log\u0026gt; Ruta de archivo para almacenar la salida estandar del job.   --error = \u0026lt;rutaArchivo.log \u0026gt;  -e \u0026lt;rutaArchivo.log\u0026gt; Ruta de archivo para almacenar errores en jobs.   --time=\u0026lt;tiempo\u0026gt;  -t \u0026lt;tiempo\u0026gt; Tiempo requerido.    Ejemplos de job scripts     Ejemplo 1  Imprime la fecha actual.\n Crear el archivo ej1.sh:  #!/bin/bash #SBATCH --job-name=ej1 # nombre del job #SBATCH -c 1 # numero de cpu cores a usar #SBATCH --mem-per-cpu=100mb # tamano de memoria del job en ejecucion. date sleep 10 # duerme 10s, solo para visualizar el job en la fila Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej1.sh   Ejemplo 2  Imprime \u0026ldquo;Hello World”.\n Crear el archivo ej2.sh:  #!/bin/bash #SBATCH --job-name=ej2 # nombre del job. #SBATCH -c 1 # numero de cpu cores a usar. #SBATCH --time=1:00 # tiempo maximo de ejecucion del job. #SBATCH --mem-per-cpu=100mb # tamano de memoria del job en ejecucion. echo \u0026#34;Hello World” # imprime Hello World sleep 10 # duerme 10s # End of script Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej2.sh   Ejemplo 3  Muestra las variables de ambiente propias de Slurm.\n Crear el archivo ej3.sh:  #!/bin/bash #SBATCH -J ej3 # nombre del job set | grep SLURM Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej3.sh   Ejemplo 3  Muestra las variables de ambiente propias de Slurm.\n Crear el archivo ej3.sh:  #!/bin/bash #SBATCH -J ej3 # nombre del job set | grep SLURM Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej3.sh    Ejemplos de job scripts con MPI, OpenMP y Python     Ejemplo 4  Ejecución del programa en Openmpi, prueba_mpi.c:\n#include \u0026lt;mpi.h\u0026gt;#include \u0026lt;stdio.h\u0026gt; int main(int argc, char** argv) { // Initialize the MPI environment MPI_Init(NULL, NULL); // Get the number of processes int world_size; MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;world_size); // Get the rank of the process int world_rank; MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;world_rank); // Get the name of the processor char processor_name[MPI_MAX_PROCESSOR_NAME]; int name_len; MPI_Get_processor_name(processor_name, \u0026amp;name_len); // Print off a hello world message printf(\u0026#34;Hello world from processor %s, rank %d out of %d processors\\n\u0026#34;, processor_name, world_rank, world_size); // Finalize the MPI environment. MPI_Finalize(); }  Compilar el programa mpi:  [juana.perez@khipu ~]$ module load openmpi/2.1.6 [juana.perez@khipu ~]$ mpicc prueba_mpi.c -o prueba_mpi [juana.perez@khipu ~]$ module unload openmpi/2.1.6 Crear el archivo ej4.sh:  #!/bin/bash #SBATCH -J ej4 # nombre del job #SBATCH -p investigacion # nombre de la particion  #SBATCH -N 2 # numero de nodos #SBATCH --tasks-per-node=3 # numero de tasks por nodo module load openmpi/2.1.6 # carga el modulo de openmpi version 2.1.6 srun prueba_mpi # siendo prueba_mpi el nombre del programa mpi module unload openmpi/2.1.6 Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej4.sh   Ejemplo 5  Ejemplo de ejecución del programa OpenMP, prueba_openmp.c:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;omp.h\u0026gt; void Hello(void); /* Thread function */ /*--------------------------------------------------------------------*/ int main(int argc, char* argv[]) { int thread_count = strtol(argv[1], NULL, 10); # pragma omp parallel num_threads(thread_count) Hello(); return 0; } /* main */ /*------------------------------------------------------------------- * Function: Hello * * Purpose: Thread function that prints message * */ void Hello(void) { int my_rank = omp_get_thread_num(); int thread_count = omp_get_num_threads(); printf(\u0026#34;Hello from thread %d of %d\\n\u0026#34;, my_rank, thread_count); /* Hello */ }  Compilar el programa prueba_openmp.c:  [juana.perez@khipu ~]$ module load gcc/5.5.0 [juana.perez@khipu ~]$ g++ prueba_openmp.c -fopenmp -lpthread -o prueba_openmp [juana.perez@khipu ~]$ module unload gcc/5.5.0 Crear el archivo ej2.sh:  #!/bin/bash #SBATCH -J ej6 #SBATCH -N 1 #SBATCH --tasks-per-node=8 #SBATCH --mem=1GB module load gcc/5.5.0 unset OMP_NUM_THREADS ./ejemplo_openmp ${SLURM_NPROCS} module unload gcc/5.5.0 Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej6.sh   Ejemplo 3  Muestra las variables de ambiente propias de Slurm.\n Crear el archivo ej3.sh:  #!/bin/bash #SBATCH -J ej3 # nombre del job set | grep SLURM Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej3.sh   Ejemplo 6  Ejecución del programa en Python, prueba_python.py:\nfrom math import factorial as f print(\u0026#34;Hola mundo\u0026#34;) N = 100 print(\u0026#34;%d! = %d\u0026#34; %(N, f(N)))  Crear el archivo ej5.sh:  #!/bin/bash #SBATCH -J ej5 # nombre del job #SBATCH -p investigacion # nombre de la particion  #SBATCH -c 1 # numero de cpu cores a usar module load python/2.7.17 # carga el modulo de python version 2.7.17 python2.7 prueba_python.py # siendo prueba_python.py el nombre del programa python module unload python/2.7.17 Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej5.sh    Ejemplos de job scripts avanzados     Ejemplo 7  Ejecución de dos jobs, donde uno depende del término del otro.\n Crear los archivos ej7-1.sh y ej7-2.sh:  #!/bin/bash #SBATCH -J ej7-1 #SBATCH -N 1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB echo \u0026#34;ej7-1\u0026#34; date sleep 120 date #!/bin/bash #SBATCH -J ej7-2 #SBATCH -N 1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB echo \u0026#34;ej7-2\u0026#34; date sleep 120 date Enviar a ejecutar los jobs:  [juana.perez@khipu ~]$ n_proc=$(sbatch ej7-1.sh) [juana.perez@khipu ~]$ sbatch -d after:${n_proc##* } ej7-2.sh [juana.perez@khipu ~]$ squeue   Ejemplo 8  Muestra como pasar variables a los jobs, esto permite setear variables de ambiente para la ejecución dentro de nuestro script. No es recomendable usar seteos de variables direcamente como comandos Unix. Existen dos formas:\n Colocar el set de variables que se necesita en el script mediante el flag “#SBATCH \u0026ndash;export=var1=valor,var2=valor…”.   Crear el archivo ej8-1.sh:  #!/bin/bash #SBATCH -J ej8-1 #SBATCH -N 1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB #SBATCH --export=var1=12 echo $var1  Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej8-1.sh Especificar el valor de las variables al momento del envío del script. Brinda flexibilidad y es recomendable usarlo cuando solo varían pocos parámetros, como el nombre de un archivo o valor único.   Crear el archivo ej8-2.sh:  #!/bin/bash #SBATCH -J ej8-2 #SBATCH -N 1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB echo $var1  Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch --export=var1=15 ej8-2.sh    Información adicional      Tutorial de Linux Documentación de Slurm  "},{"id":21,"href":"/guia_de_usuario/ejecutando_trabajos/monitorear_jobs/","title":"Monitorear Jobs","parent":"Ejecutando Jobs","content":""},{"id":22,"href":"/guia_de_usuario/ejecutando_trabajos/scheduler_ejemplos/","title":"Ejemplos de Scheduler","parent":"Ejecutando Jobs","content":""},{"id":23,"href":"/guia_de_usuario/ejecutando_trabajos/scheduler_configuracion/","title":"Configuración de Scheduler","parent":"Ejecutando Jobs","content":""},{"id":24,"href":"/guia_de_usuario/transfiriendo_data/","title":"Transfiriendo Data","parent":"Guía de Usuario","content":""},{"id":25,"href":"/guia_de_usuario/configuracion_de_hardware/","title":"Configuracion de Hardware","parent":"Guía de Usuario","content":""},{"id":26,"href":"/ayuda/","title":"Ayuda","parent":"Inicio","content":"    Solicitando Ayuda     ¿Cómo pedir ayuda?     Preguntas que podemos atender     Recomendaciones       FAQs     Tutoriales     "},{"id":27,"href":"/ayuda/solicitando_ayuda/","title":"Solicitando Ayuda","parent":"Ayuda","content":""},{"id":28,"href":"/ayuda/solicitando_ayuda/como_pedir_ayuda/","title":"¿Cómo pedir ayuda?","parent":"Solicitando Ayuda","content":""},{"id":29,"href":"/ayuda/solicitando_ayuda/preguntas_a_atender/","title":"Preguntas que podemos atender","parent":"Solicitando Ayuda","content":""},{"id":30,"href":"/ayuda/solicitando_ayuda/recomendaciones/","title":"Recomendaciones","parent":"Solicitando Ayuda","content":""},{"id":31,"href":"/ayuda/faqs/","title":"FAQs","parent":"Ayuda","content":""},{"id":32,"href":"/ayuda/tutoriales/","title":"Tutoriales","parent":"Ayuda","content":""},{"id":33,"href":"/reportes/","title":"Reportes","parent":"Inicio","content":"Importante: Información actualizada cada 2 minutos.  Reporte General     Reporte de CPU      Reporte de Memoria     Reporte de Red       "},{"id":34,"href":"/anuncios/bienvenida/","title":"Bienvenida","parent":"Anuncios","content":"Lorem Ipsum \u0026ldquo;Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit\u0026hellip;\u0026rdquo; \u0026ldquo;There is no one who loves pain itself, who seeks after it and wants to have it, simply because it is pain\u0026hellip;\u0026rdquo;\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris pretium ligula eget metus sodales, id tincidunt tortor sollicitudin. Donec condimentum enim neque, placerat ornare est pretium at. Nulla ultrices dictum sem, at tempus nisl. Duis vitae euismod eros, eu scelerisque ante. Vestibulum dignissim leo a ipsum viverra, ut suscipit ex tempor. Cras consequat euismod nibh, ac molestie ligula varius et. Morbi a nisi at est ullamcorper mattis. Etiam iaculis felis quis nisi pellentesque, vitae iaculis nisi maximus. Phasellus venenatis non tellus vitae convallis. Donec quis cursus erat. Donec quis velit sed eros semper eleifend eget vel purus. Nullam scelerisque enim ac dapibus condimentum. Curabitur nec risus est.\nDonec at vestibulum felis. Mauris et nisl ut erat volutpat aliquet. Morbi semper risus mi, quis egestas orci sodales non. Nam rutrum commodo lectus ac maximus. Cras convallis lacus eget nulla sodales convallis. In auctor varius ex. Morbi eget malesuada magna. Aenean non pharetra velit, et blandit magna. Cras molestie sapien sed mauris rutrum molestie. Donec orci arcu, tempus a lorem non, feugiat pellentesque erat. Praesent eros nunc, dapibus id facilisis vitae, auctor ut quam. Sed justo est, dapibus ac enim ut, varius porttitor orci. Suspendisse vitae dolor vehicula, vestibulum lectus ac, dapibus odio.\nDonec ut velit id est bibendum posuere. Suspendisse cursus tempor tincidunt. Vestibulum consectetur orci eleifend, aliquam eros vitae, pulvinar nibh. Integer quis dictum quam, ut vehicula risus. Etiam rutrum placerat aliquet. Phasellus cursus tellus eu mi scelerisque, non suscipit lectus aliquet. Suspendisse elementum laoreet fringilla. Ut molestie vitae odio at accumsan. Cras vel eros quam. Donec ex metus, faucibus nec volutpat non, commodo ut turpis.\nQuisque ultricies efficitur tortor, eu viverra arcu consectetur varius. Nunc sed molestie diam. Aliquam a nisl sed magna mollis volutpat eget in dui. Donec efficitur odio a libero malesuada tincidunt. Duis vitae odio non quam pulvinar porta. In et imperdiet augue. Sed ornare eleifend nisi. Quisque aliquam consequat metus nec luctus.\nIn ut risus a nibh laoreet elementum. Nam sed nulla ipsum. Nunc pharetra vestibulum erat ac tempor. Quisque arcu neque, ultricies eget elementum sit amet, tempor faucibus turpis. Suspendisse lobortis rutrum pharetra. Nulla mollis facilisis lectus, eu luctus quam congue in. Sed congue facilisis sem at efficitur. Cras nec urna ligula. Interdum et malesuada fames ac ante ipsum primis in faucibus. Nam nibh ante, porttitor vel fermentum sed, fermentum eget elit. Generated 5 paragraphs, 388 words, 2582 bytes of Lorem Ipsum\n"},{"id":35,"href":"/","title":"Inicio","parent":"","content":"Bienvenido a la documentación de Khipu   Esta es la documentación oficial del cluster Khipu. En este sitio encontrarás toda la información sobre el cluster, las políticas y guías de uso.\nIniciar   Enlaces Rápidos   Primeros Pasos   Si no sabe por donde empezar con la documentación, este es un buen punto de comienzo.  Anuncios   La información actualizada más relevante correspondiente al funcionamiento del cluster está aquí.  Reportes   Información actualizada cada dos minutos sobre el estado actual del cluster está aquí.   Zero initial configuration   Getting started in minutes. The theme is shipped with a default configuration and works out of the box.  Handy shortcodes   We included some (hopefully) useful custom shortcodes so you don\u0026rsquo;t have to and can focus on writing amazing docs.  Dark mode   Powerful dark mode that detects your system preferences or can be controlled by a toggle switch.   Sobre Khipu   Khipu es un cluster dedicado a la computación de alto desempeño, en inglés High performance Computing (HPC). Está formado por una colección de servidores distribuídos, llamados nodos que se encuentran conectados a través de una interconexión de alta velocidad (Infiniband).\nKhipu es parte del Centro de Investigación en Computación Sostenible (COMPSUST) de la Universidad de Ingeniería y Tecnología (UTEC).\n Contacto: khipu@utec.edu.pe.\nPara información de acceso revisar aquí.\n Grupos de usuarios   Existen dos grupos de usuarios gerenciados automáticamente por Slurm: 1) investigación, y 2) educación. Ambos grupos acceden al cluster por el nodo líder para enviar trabajos a la fila de ejecución.\nGrupo Educación   Permite el uso del cluster para aula o laboratorio por los estudiantes a pedido de un instructor(a) registrado(a). Este grupo es financiado por la universidad y sus trabajos serán procesados bajo las siguientes características:\n Ocupar un nodo CPU y/o GPU. Equivalente a un total de 72 cores, 320 GB de memoria y 1.4 TB de almacenamiento.\n Tiempo máximo de ejecución de trabajo: 2 horas.\n Número de procesos concurrentes: 2.\n Cantidad de cores por proceso 8.  Grupo Investigación   Este grupo está financiado por UTEC, proyectos de investigación (PI), Departamentos y Dirección de Escuela. Los fondos centrales cubren costos de infraestructura, operación y soporte. Los PI y algunas unidades y departamentos financian la adquisición de nuevos nodos de procesamiento y almacenamiento. Los usuarios de este grupo ejecutan trabajos en todos los nodos bajo las siguientes características:\n Ocupar cualquier nodo CPU y/o GPU. Equivalente a un total de 200 cores, 1.1 TB de memoria y 5.3 TB de almacenamiento.\n Tiempo máximo de ejecución de trabajo NaN.\n Número de procesos concurrentes NaN.\n Cantidad de cores por proceso NaN.   Infraestructura   Para mayor información sobre programas, ver Software\n .myTable { border-radius: 5px; } .myTable th { background-color:var(--header-background); color: white; }     nLíder     Nodo de acceso al cluster, usado principalmente para compilar y enviar trabajos.   Procesadores: Intel(R) Xeon(R) Gold 6230 CPU @ 2.10 GHz 20 cores por socket, 40 por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 480 SSD, total 40 TB HDD   Network: Infiniband FDR MT4119       nCPU (1, 2, 3, 4, 5)     Usado para procesamiento, gerenciado automáticamente por Slurm.   Procesadores: Intel(R) Xeon(R) Gold 6130 CPU @2.10 GHz 16 cores por socket, 32 por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 960 GB SSD   Network: Infiniband FDR MT4119       nGPU 1     Usado para procesamiento, gerenciado automáticamente por Slurm.   Procesadores: Intel(R) Xeon(R) Gold 6230 CPU @2.10 GHz 20 cores por socket, 40 por nodo.   Gráficos: NVIDIA Tesla T4 16 GB GDDR6, PCIe 3.0 x16 1 GPU por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 480 GB SSD   Network: Infiniband FDR MT4119       nGPU 2     Usado para procesamiento, gerenciado automáticamente por Slurm.   Procesadores: AMD EPYC 7742 @2.24 GHz 64 cores por socket, 128 por nodo.   Gráficos: NVIDIA Tesla T4 16 GB GDDR6, PCIe 3.0 x16 1 GPU por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 480 GB SSD   Network: Infiniband FDR MT4119     --   "},{"id":36,"href":"/tags/","title":"Tags","parent":"Inicio","content":""}]