[{"id":0,"href":"/overview/","title":"Informaci√≥n","parent":"Inicio","content":"    Primeros Pasos      Sobre Khipu      Infraestructura      Galer√≠a      "},{"id":1,"href":"/cuentas/","title":"Cuentas","parent":"Inicio","content":"    Grupos de Cuentas      Solicitud de Acceso      Pol√≠ticas      "},{"id":2,"href":"/overview/primeros_pasos/","title":"Primeros Pasos","parent":"Informaci√≥n","content":"    Clusters HPC Khipu Solicitar una cuenta Iniciar Sesi√≥n Enviar un Job Linux Enviar Archivos Uso de Software Reglas de uso Obtener ayuda      Clusters HPC Khipu     Visto de manera sencilla, Khipu es una colecci√≥n de computadoras que que se encuentran conectadas entre s√≠ y que trabajan como si fueran una sola. A cada computador conectado se le conoce como nodo. El nodo de acceso, o tambi√©n llamado nodo l√≠der, es aquel al cual nos conectamos de manera remota para acceder al cluster. Los nodos que se encargan de la ejecuci√≥n de nuestros jobs o trabajos, son llamados nodos de c√≥mputo. Para enviar un job hacemos uso de un scheduler, el cual se encargado de manejar los diferentes jobs que se env√≠an y garantizar equidad al momento que pasan a ejecutarse. Todos los nodos de computo, poseen un filesystem compartido lo cual permite a los jobs acceder y modificar la data de cualquier nodo sin la necesidad de copiarlo al otro.\nSolicitar una cuenta     Para poder acceder al cluster Khipu es necesario solicitar una cuenta. Las cuentas se dividen en dos grupos: educaci√≥n e investigaci√≥n. Las solicitudes de cuentas no implican costo alguno para los miembros de UTEC y pueden ser solicitadas por el docente del curso o el encargado del proyecto de investigaci√≥n. Mayores detalles sobre las cuentas en Khipu lo encuentra aqu√≠.\nAbrir solicitud de acceso    Una vez completado el formulario y aprobada la solicitud de acceso, las credenciales ser√°n enviadas a los correos registrados. A partir del momento en que se reciben las credenciales, el usuario se compromete a cumplir las pol√≠ticas de uso.\nIniciar Sesi√≥n     Una vez adquirida una cuenta, puede acceder al cluster por terminal via SSH. Dir√≠jase al apartado de inicio de sesi√≥n para conocer c√≥mo realizar este proceso.\nEnviar un Job     Dentro del cluster, usted puede enviar a ejecuci√≥n sus trabajos o jobs a trav√©s de un scheduler. Una vez adquirida una cuenta, puede acceder al cluster por terminal via SSH. Dir√≠jase al apartado de enviar jobs para conocer c√≥mo realizar este proceso.\nLinux     Enviar Archivos     Uso de Software     Reglas de uso     Obtener ayuda     "},{"id":3,"href":"/anuncios/","title":"Anuncios","parent":"Inicio","content":""},{"id":4,"href":"/cuentas/cuentas_del_cluster/","title":"Grupos de Cuentas","parent":"Cuentas","content":"Existen dos grupos de usuarios gerenciados autom√°ticamente por Slurm: 1) educaci√≥n, y 2) investigaci√≥n. Ambos grupos acceden al cluster por el nodo l√≠der para enviar trabajos a la fila de ejecuci√≥n.\nGrupo Educaci√≥n     Permite el uso del cluster para aula o laboratorio por los estudiantes a pedido de un instructor(a) registrado(a). Este grupo es financiado por la universidad y sus trabajos ser√°n procesados bajo las siguientes caracter√≠sticas:\n Ocupar un nodo CPU y/o GPU. Equivalente a un total de 72 cores, 320 GB de memoria y 1.4 TB de almacenamiento.\n Tiempo m√°ximo de ejecuci√≥n de trabajo: 2 horas.\n N√∫mero de procesos concurrentes: 2.\n Cantidad de cores por proceso 8.  Grupo Investigaci√≥n     Este grupo est√° financiado por UTEC, proyectos de investigaci√≥n (PI), Departamentos y Direcci√≥n de Escuela. Los fondos centrales cubren costos de infraestructura, operaci√≥n y soporte. Los PI y algunas unidades y departamentos financian la adquisici√≥n de nuevos nodos de procesamiento y almacenamiento. Los usuarios de este grupo ejecutan trabajos en todos los nodos bajo las siguientes caracter√≠sticas:\n Ocupar cualquier nodo CPU y/o GPU. Equivalente a un total de 200 cores, 1.1 TB de memoria y 5.3 TB de almacenamiento.\n Tiempo m√°ximo de ejecuci√≥n de trabajo NaN.\n N√∫mero de procesos concurrentes NaN.\n Cantidad de cores por proceso NaN.  "},{"id":5,"href":"/cuentas/solicitud_de_acceso/","title":"Solicitud de Acceso","parent":"Cuentas","content":"Para poder obtener credenciales de acceso a Khipu, es necesario completar un formulario con informaci√≥n relevante del proyecto o curso. Este formulario deber√° ser completado por: a) el investigador principal del proyecto (PI), o b) el profesor responsable del curso.\nEl formulario es ligeramente distinto para cada grupo de cuentas. Sin embargo, en ambos casos se deber√° completar el nombre del proyecto o curso, la fecha de inicio y fin, el investigador o docente a cargo, los miembros del curso o proyecto y la lista de software licenciado o de c√≥digo abierto a emplearse. Esta √∫ltima informaci√≥n nos es √∫til para poder identificar los softwares con mayor demanda y as√≠ crear los m√≥dulos correspondientes.\nPuede acceder a la solicitud acceso a trav√©s del siguiente enlace:\nAbrir solicitud de acceso    Tambi√©n se puede acceder al formulario haciendo click a la opci√≥n Registro dentro de https://web.khipu.utec.edu.pe/\n Una vez completado el formulario, se obtendr√° respuesta a la solicitud en un plazo no mayor de 2 d√≠as h√°biles. Si la solicitud es favorable, tambi√©n se realizar√° el env√≠o de las credenciales de acceso a los correos electr√≥nicos de cada miembro registrado. Una vez que un usuario recibe sus credenciales se compromete a cumplir con las pol√≠ticas de uso. Por otro lado, si la solicitud no es favorable, se enviar√° por correo las razones de la decisi√≥n.\n"},{"id":6,"href":"/cuentas/politica/","title":"Pol√≠ticas","parent":"Cuentas","content":"    Sobre las cuentas Sobre el uso Incumplimiento de las reglas de uso Agradecimientos por el uso de Khipu Comunicaci√≥n      El uso del cluster HPC Khipu esta sujeto al cumplimiento de sus pol√≠ticas de uso. Una vez que se reciben las credenciales de acceso, los usuarios aceptan cumplir las siguiente pol√≠ticas.\nSobre las cuentas      Solo los usuarios registrados pueden acceder a Khipu. Asimismo, los usuarios se hacen responsables de tomar las precauciones necesarias para proteger sus credenciales de acceso y as√≠ impedir accesos no autorizados. Los usuarios se encuentran prohibidos de compartir sus credenciales con otras personas, as√≠ sean estos estudiantes o colaboradores. Del mismo modo, los usuarios se encuentran prohibidos de acceder a Khipu con credenciales que no son suyas, con o sin consentimiento del usuario. En caso de existir sospecha que otros han usado su cuenta, notificarlo inmediatamente a khipu@utec.edu.pe. Las cuentas ser√°n desactivadas si una de las siguientes condiciones se cumple:  La fecha de fin del curso o proyecto ha sido alcanzada, o El encargado del curso o proyecto indica que el usuario ya no requiere acceso, o El usuario ha recibido alguna sanci√≥n disciplinaria grave de parte de la universidad.    Sobre el uso     El cluster Khipu es un recurso compartido por multiples usuarios, as√≠ que las acciones que usted realice pueden afectar a otros usuarios si no se realizan de manera adecuada. Es por ello, que se brindan las siguientes recomendaciones a fin de garantizar un uso equitativo y justo de los recursos del cluster.\n El uso del cluster se encuentra restringido solo para fines educativos y de investigaci√≥n. Su uso no debe estar relacionado a actividades comerciales, de consultor√≠a o creaci√≥n de software malicioso. Los usuarios son responsables de usar los recursos del cluster de manera eficiente, efectiva, √©tica y l√≠cita.   El nodo de administraci√≥n es usado para acceder al cluster, editar archivos, compilar c√≥digo y enviar trabajos. De ninguna manera, se deben ejecutar los trabajos directamente en el nodo de administraci√≥n. Toda ejecuci√≥n de trabajos debe ser realizada a traves del gestor de recursos slurm. Es as√≠ como se garantizan un uso eficiente y justo de los recursos. El cluster no debe ser utilizado como almacenamiento personal. Se recomienda copiar sus resultados a su m√°quina personal una vez culminado el uso del cluster. Los usuarios no deben intentar acceder a cualquier archivo o programa al cual no poseen autorizaci√≥n o un consentimiento explicito del due√±o del archivo o programa. Los usuarios pueden hacer uso de las aplicaciones ya instaladas, y tambi√©n compilar nuevas aplicaciones en su espacio personal solamente si fueran necesarios para el desarrollo de su proyecto o trabajo. Los softwares que se instalen deben incluir una licencia v√°lida (si es aplicable). No se puede instalar software de procedencia ilegal y/o con licencia no otorgada por los desarrolladores. La presente pol√≠tica ser√° revisada y actualizada de manera peri√≥dica. Cualquier cambio ser√° comunicado al correo electr√≥nico registrado.  Incumplimiento de las reglas de uso     El usuario acepta cumplir con la normativa y sanciones impuestas por UTEC en sus atribuciones.\n En caso de no cumplir normas de UTEC y/o atentar contra la ley, el evento ser√° notificado a las autoridades pertinentes. En caso de incumplimiento de los items anteriores, la cuenta ser√° suspendida.  Agradecimientos por el uso de Khipu     Alentamos a todos a los usuarios cuyo proyecto termina en una publicaci√≥n o presentaci√≥n a a√±adir al cluster Khipu dentro de los agradecimientos. Nos ayudar√≠a de sobremanera, mencionar el rol que tuvo Khipu a lo largo de su investigaci√≥n o formaci√≥n acad√©mica. Su sola menci√≥n contribuye a comunicar el rol de Khipu en el desarrollo de investigaciones dentro de la universidad, incentiva a m√°s estudiantes y docentes a incorporar a Khipu dentro de sus proyectos o planes de estudio, mantiene el financiamiento y soporte que hace posible que Khipu siga creciendo cada d√≠a m√°s. Puede a√±adir un mensaje de agradecimiento como el siguiente:\n \u0026ldquo;El trabajo computacional del presente producto fue apoyado por los recursos del cluster HPC Khipu (https://web.khipu.utec.edu.pe/) de la Universidad de Ingenier√≠a y Tecnolog√≠a (UTEC) \u0026quot;\n Comunicaci√≥n     El correo electr√≥nico ingresado al momento de solicitar acceso a Khipu ser√° el medio por el cual se enviar√°n anuncios relevantes al funcionamiento del cluster. Si usted presenta preguntas, pedidos o requiere asistencia puede escribir de manera directa a khipu@utec.edu.pe. Por favor, aseg√∫rese de seguir las recomendaciones de soporte a fin de poder atender sus requerimientos de mejor manera.\n"},{"id":7,"href":"/cuentas/recomendaciones/","title":"Recomendaciones","parent":"Cuentas","content":""},{"id":8,"href":"/guia_de_usuario/","title":"Gu√≠a de Usuario","parent":"Inicio","content":"    Iniciar Sesi√≥n      Software      Software Disponible      Usando Software      Instalando Software        Jobs Schedulling      Introducci√≥n      Enviar Jobs      Monitorear Recursos      Ejemplos      Introductorios      MPI y OpenMP      Python      Conda          Transfiriendo Data      Configuracion de Hardware     "},{"id":9,"href":"/overview/khipu/","title":"Sobre Khipu","parent":"Informaci√≥n","content":"    Sobre Khipu  ¬øQu√© es computaci√≥n de alto desempe√±o?        Sobre Khipu     Khipu es un cluster de computaci√≥n de alto desempe√±o que forma Centro de Investigaci√≥n para la Computaci√≥n Sostenible (COMPSUST) y el Departamento de Ciencia de la Computaci√≥n (CS) de la Universidad de Ingenier√≠a y Tecnolog√≠a (UTEC).\n Misi√≥n\nSomos un grupo acad√©mico que apostamos por el desarrollo cient√≠fico de nuestra comunidad, a traves de la computaci√≥n de alto desempe√±o.\n   Visi√≥n\nSer un laboratorio referente en computaci√≥n de alto desempe√±o a nivel local, y ser reconocidos por nuestra colaboraci√≥n al desarrollo cient√≠fico de nuestra comunidad.\n   ¬øQu√© es computaci√≥n de alto desempe√±o?     La computaci√≥n de alto desempe√±o, o HPC por sus siglas en ingl√©s, es la pr√°ctica de agregar poder computacional de diferentes ordenadores, de tal manera que se obtiene un desempe√±o mucho mayor al de un ordenador convencional aprovechando el poder del paralelismo. Todo ello con el objetivo de resolver de manera m√°s r√°pida los problemas complejos de campos como matem√°ticas, ciencias e ingenier√≠as.\nHPC nos permite resolver problemas complejos que tardar√≠an a√±os, meses o semanas para resolverse en un computador convencional, pero que con el uso de HPC, se podr√≠a reducir a d√≠as, horas o minutos.\nLeer m√°s:\n What is High Performance Computing?  What is High-Performance Computing (HPC)?  "},{"id":10,"href":"/guia_de_usuario/iniciar_sesion/","title":"Iniciar Sesi√≥n","parent":"Gu√≠a de Usuario","content":"    Inicio de sesi√≥n regular Inicio de sesi√≥n con SSH Keys  Linux o MacOS Windows        Inicio de sesi√≥n regular     Para poder iniciar sesi√≥n en Khipu deberemos seguir los siguientes pasos.\n Abrir un terminal y escribir el siguiente comando reemplazando su nombre de usuario en el campo .  ssh \u0026lt;username\u0026gt;@khipu.utec.edu.pe Ejemplo:\nssh juana.perez@khipu.utec.edu.pe Aparecer√° el siguiente di√°logo. Aceptar escribiendo yes y presionar enter. Luego colocar el password (este no aparecer√° en pantalla al escribir) y presionar enter.  Are you sure you want to continue connecting (yes/no)? juana.perez@khipu.utec.edu.pe\u0026#39;s password: Al lograr accesar al cluster aparecer√° el prompt del terminal remoto.  Last login: Fri Aug 21 11:28:13 2020 from 190.236.197.233 [juanaperez@khipu ~]$ Para cerrar o salir del cluster:  [juana.perez@khipu ~]$ exit Inicio de sesi√≥n con SSH Keys     Sobre las SSH Keys ‚Üï  ¬øQu√© son las SSH Keys?     SSH o Secure Shell Keys son un conjunto de informaci√≥n que se emplea para identificar y encriptar la comunicaci√≥n entre su m√°quina local y un servidor. Se compone de dos archivos: una clave p√∫blica (id_rsa.pub) y otra privada (id_rsa o id_rsa.ppk). Se manera simple la clave p√∫blica es un \u0026ldquo;candado\u0026rdquo; y la clave privada es la llave para abrir dicho \u0026ldquo;candado\u0026rdquo;. El \u0026ldquo;candado\u0026rdquo; (llave p√∫blica) puede ser compartida sin mayores problemas, en cambio si comparte su llave privada, alguien podr√≠a suplantar su identidad.\nCuando se conecta a un servidor, este presentar√° su llave p√∫blica. Usted provar√° su identidad mediante el uso de su llave privada. La comunicaci√≥n con el servidor remoto y toda la informaci√≥n enviado durante este proceso ser√° encriptada mediante su llave p√∫blica, de tal manera que solo usted pueda desencriptarla usando su llave privada.\n  Si desea acceder a Khipu sin escribir su contrase√±a deber√° copiar la llave p√∫blica de su computador al cluster. La llave p√∫blica debe haber sido creada previamente con el siguiente comando.\nssh-keygen Su terminal responder√° con:\nGenerating public/private rsa key pair. Enter file in which to save the key (/home/username/.ssh/id_rsa): Presione Enter para aceptar el valor por defecto. Su terminal responder√° con:\nEnter passphrase (empty for no passphrase): Usted deber√° eligir un passphrase seguro. Este valor le ayudar√° a prevenir el acceso a su cuenta en caso su clave privada es robada. Mientras escribe su passphrase este no aparecer√° en pantalla. Su terminal responder√° con:\nEnter same passphrase again: Ingrese el passphrase nuevamente. Una vez hecho esto se generar√° el par de llaves dentro de un directorio .ssh en su directorio /home. Si usted olvida su passphrase, no podr√° recuperarlo. En cambio, usted deber√° generar y copiar al servidor un nuevo par de SSH Keys.\nA continuaci√≥n se muestra como copiar su clave p√∫blica a Khipu desde diferentes sistemas operativos:\nLinux o MacOS     ssh-copy-id -i ~/.ssh/id_rsa.pub juana.perez@khipu.utec.edu.pe Windows     type %HOMEPATH%\\.ssh\\id_rsa.pub | ssh juana.perez@khipu.utec.edu.pe \u0026#34;mkdir .ssh; cat \u0026gt;\u0026gt; .ssh/authorized_keys\u0026#34; "},{"id":11,"href":"/guia_de_usuario/software/","title":"Software","parent":"Gu√≠a de Usuario","content":"    Software Disponible      Usando Software      Instalando Software      "},{"id":12,"href":"/overview/infraestructura/","title":"Infraestructura","parent":"Informaci√≥n","content":"    Vista general Nodo de administraci√≥n Nodos de computaci√≥n  Nodos CPU Nodos GPU   Software del sistema Aplicaciones de software      Vista general      .myTable { border-radius: 5px; } .myTable th { background-color:var(--header-background); color: white; }  Nodo de administraci√≥n     Nodo de acceso al cluster, usado principalmente para compilar y enviar trabajos.\n   Especificaciones     Nombre nL√≠der   Procesador Intel(R) Xeon(R) Gold 6230 CPU @ 2.10 GHz 20 cores por socket, 40 por nodo.    Memoria DRAM DDR4-1333 MHz, 128 GB por nodo    Almacenamiento 480 SSD, total 40 TB HDD    Red Infiniband FDR MT4119    Nodos de computaci√≥n     Usado para procesamiento, gerenciado autom√°ticamente por Slurm.\nNodos CPU        Especificaciones     Nombre nCPU   Procesador Intel(R) Xeon(R) Gold 6130 CPU @2.10 GHz 16 cores por socket, 32 por nodo.   Memoria DRAM DDR4-1333 MHz, 128 GB por nodo    Almacenamiento 960 GB SSD   Red Infiniband FDR MT4119    Nodos GPU        Especificaciones     Nombre nGPU   Procesador Intel(R) Xeon(R) Gold 6230 CPU @2.10 GHz 20 cores por socket, 40 por nodo.    Gr√°ficos NVIDIA Tesla T4 16 GB GDDR6, PCIe 3.0 x16 1 GPU por nodo.   Memoria  DRAM DDR4-1333 MHz, 128 GB por nodo    Almacenamiento 480 GB SSD   Red Infiniband FDR MT4119     Software del sistema      Sistema Operativo: CentOS Linux 7 Message Passing Library: MPICH Compiladores: Intel, GCC Job Scheduler: SLURM Manejo de software: M√≥dulos de ambiente  Aplicaciones de software     Revise aqu√≠ la lista de softwares, herramientas y utilidades disponible en Khipu.\n"},{"id":13,"href":"/guia_de_usuario/software/software_disponible/","title":"Software Disponible","parent":"Software","content":"    M√≥dulos disponibles      Para poder satisfacer las diferentes necesidades de nuestros usuarios, hacemos uso de sotfware modules a fin de poder hacer disponibles multiples versiones de software. Los m√≥dulos ayudan a cambiar entre diferentes aplicaciones y sus versiones con relativa facilidad, especialmente en ambientes compartidos.\nM√≥dulos disponibles     Usted puede listar los m√≥dulos que se encuentran disponibles con el siguiente comando.\nmodule avail A continuaci√≥n se comparte una muestra de la salida del comando anterior.\n--------------------------------------- /opt/apps/modulefiles ---------------------------------------- armadillo/10.8.2 go/1.17.6 mpich/4.0 brams/5.3 gromacs/2020.1 namd/2019.07.11-multicore cmake/3.16.5 gromacs/2020.3 namd/2022.04.05-multicore cmake/3.22.2 gromacs/gpu-2020.1 namd/2022.04.05-multicore-CUDA cuda/10.2 gromacs/gpu-2020.3 openfoam/v2112 cuda/11.4 intel-oneapi/2022.1.2 pmix/2.1 gaussian/1.1.0 julia/1.5.3 python/2.7.17 gcc/10.1.0 meep/1.17.1 python/3.7.7 gcc/5.5.0 metis/5.1.0 python/3.9.2 gcc/6.5.0 mpich/1.5 speedtest/latest gcc/7.5.0 mpich/3.1.4 telemac/8.3.0 gcc/8.4.0 mpich/3.2.1 valgrind/3.15.0 gcc/9.2.0 mpich/3.3.2 valgrind/3.18.1 go/1.14.6 mpich/3.4.0 wrf/4.2 A continuaci√≥n se muestra la lista de software disponible por categor√≠as.\n .myTable { border-radius: 5px; } .myTable th { background-color:var(--header-background); color: white; }     Categor√≠a Componente     Sistema Operativo CentOS Linux 7.6 x86_64   Gestor de trabajos Slurm   Herramientas de desarrollo   Editores de texto Vim, Nano   Sistema de Control de Versiones Git   Compiladores GNU (gcc, g++, gfortran), CMake, Intel    Lenguajes C, C++, Python, R, Java, Go, Julia   Debugging, profiling, tracing GDB, CGDB, Valgrind   Distribuci√≥n MPI MPICH   GPU CUDA   Otros   Ingenier√≠a OpenFOAM   Geo-sotfware WRF, BRAMS, Telemac   Bioinform√°tica NAMD    "},{"id":14,"href":"/guia_de_usuario/software/usando_software/","title":"Usando Software","parent":"Software","content":"    M√≥dulos  Buscar m√≥dulos Cargar m√≥dulos Descargar m√≥dulos Colecci√≥n de m√≥dulos Obtener informaci√≥n y ayuda de los m√≥dulos        M√≥dulos     Buscar m√≥dulos     Usted tambi√©n puede buscar m√≥dulos mediante el comando avail. Por ejemplo, para listar todas las versiones de MPICH 3, ejecutaremos:\nmodule avail mpich/3 Y obtendremos una salida similar a:\n--------------------------------------- /opt/apps/modulefiles ---------------------------------------- mpich/1.5 mpich/3.1.4 mpich/3.2.1 mpich/3.3.2 mpich/3.4.0 mpich/4.0 Cargar m√≥dulos     Usted puede cargar un modulo a su ambiente de trabajo a traves del comando module load. Al cargar un modulo, usted carga todas las variables de ambiente necesarias para poder utilizar dicho paquete de software.\nEs importante tomar en cuenta que:\n El comando es case-sensitive a los nombres de los m√≥dulos Cuando usted usa module load se cargan tambi√©n las dependencias del modulo. No es necesario cargarlas de manera separada. Cuando usted envi√° sus jobs que requieren de un m√≥dulo, no olvide a√±adirlo con module load a su bash script.  Por ejemplo, si desea cargar gcc versi√≥n 7.5.0, deber√° ejecutar:\nmodule load gcc/7.5.0 Tambi√©n puede cargar varios m√≥dulos al mismo tiempo:\nmodule load gcc/7.5.0 mpich/3.3.2 Descargar m√≥dulos     Una vez terminado de usar el software, es recomendable hacer module unload \u0026lt;modulename\u0026gt;. Este proceso tambi√©n se debe realizar e los scripts de los jobs.\nmodule unload gcc/7.5.0 Tambi√©n puede descargar todos los m√≥dulos a la vez con:\nmodule purge Colecci√≥n de m√≥dulos     En casos cuando se trabaja con gran cantidad de m√≥dulos, puede ser molestoso estar cargando cada uno de ellos. Ante esto existe la opci√≥n de a√±adir dicho m√≥dulos a una colecci√≥n y de este modo solo cargar la colecci√≥n en lugar que todos los m√≥dulos de manera individual.\nGuardar colecciones\nPara ello, usted deber√° cargar previamente los m√≥dulos que desea a√±adir a la colecci√≥n y despues ejecutar.\nmodule save \u0026lt;collection-name\u0026gt; Cargar colecciones\nUsted podr√° cargar la colecci√≥n creada con:\nmodule restore \u0026lt;collection-name\u0026gt; Listar colecciones\nPara listar las colecciones creadas:\nmodule savelist Obtener informaci√≥n y ayuda de los m√≥dulos     Si desea mostrar la informaci√≥n de configuraci√≥n de un modulo en especifico:\nmodule show \u0026lt;modulename\u0026gt; #or module display \u0026lt;modulename\u0026gt; Usted puede obtener una breve descripci√≥n sobre un m√≥dulo ejecutando:\nmodule help \u0026lt;modulename\u0026gt;/\u0026lt;version\u0026gt; Si usted no encuentra un paquete y considera que su instalaci√≥n como modulo podr√≠a ser beneficiosa para otros usuarios m√°s, puede escribirnos a khipu@utec.edu.pe para solicitar su instalaci√≥n.\n  "},{"id":15,"href":"/guia_de_usuario/software/instalando_software/","title":"Instalando Software","parent":"Software","content":"    Requisitos para instalar software Proceso de instalaci√≥n      El siguiente documento describe como instalar software en Khipu para un proyecto o curso. Antes de realizar una instalaci√≥n, por favor revise si el software ya se encuentra disponible como m√≥dulo con el comando module avail\nRequisitos para instalar software      En caso de software licenciado, el usuario debe proveer la licencia y/o instalador del software al administrador para ser instalado. El usuario puede instalar el software y libraries necesarios en su /home/\u0026lt;username\u0026gt;/, en caso de no requerir licencia o ser de c√≥digo abierto. El usuario puede compilar en el nodo l√≠der antes de enviar un job a ejecutar. No enviar jobs de compilaci√≥n a la fila del cluster. Siendo Khipu un cluster enfocado al procesamiento y HPC; no se instalar√°n bases de datos, containers o m√°quinas virtuales. Si el software o library requiere permisos especiales para instalaci√≥n, solamente el profesor encargado del curso o un investigador de proyecto pueden solicitar la instalaci√≥n bajo coordinaci√≥n con el administrador.   Si el software que planea instalar posee dependencias como librer√≠as, interfaces o m√≥dulos; revise primero si ya se encuentran disponibles en el cluster antes de instalarlas por su cuenta.  Para entrar en contacto con el administrador del cluster, por favor enviar un mensaje a khipu@utec.edu.pe.\nPor favor entrar en contacto en caso se requiera un software o library adicional que va a ser usado por m√∫ltiples investigadores o participantes de un curso que no ha sido especificado en el Formulario de Solicitud de Acceso. -- Proceso de instalaci√≥n     Tal como se mencion√≥ anteriormente, usted puede instalar software libre o que no requiere licencia en su espacio personal de trabajo. Sin embargo, antes de proceder con la instalaci√≥n aseg√∫rese que dicho software no requiere permisos root para ser instalado. Una vez echo esto, usted deber√° moverse al directorio en el cual planea realizar la instalaci√≥n y seguir las instrucciones del proveedor del software que planea instalar.\nPor lo general, los pasos son similares, es por ello que a continuaci√≥n se muestra un ejemplo realizando la instalaci√≥n de la librer√≠a Zlib, un software usado para comprimir y descomprimir data.\n Creamos un directorio en el cual almacenaremos los c√≥digos fuente y binarios de las apps que vamos a instalar.  mkdir myApps cd myApps APPS_DIR=$(pwd) mkdir sources # source files mkdir apps # binary files  Entraremos al directorio sources/ y descargaremos el c√≥digo fuente del paquete desde su sitio web.  cd $APPS_DIR/sources wget https://www.zlib.net/zlib-1.2.12.tar.xz  Descomprimimos el archivo descargado.  tar -xvf zlib-1.2.12.tar.xz  Entramos al nuevo directorio con los contenidos del paquete que acabamos de descomprimir.  cd zlib-1.2.12/  Cargamos el m√≥dulo del compilador para poder crear los binarios a partir del c√≥digo fuente. En este caso cargaremos GCC 7.5.0, sin embargo es recomendable seguir los requisitos de cada software.  module load gcc/7.5.0  Por lo general, muchos c√≥digo fuentes se compilan usando simplemente make y make install. Sin embargo, nosotros debemos configurar previamente el lugar donde se guardaran los binarios, ya que si no hacemos esto, se guardaran en los directorios /usr/lib a los cuales no tenemos acceso. Por lo general, bastar√° con configurar el argumento --prefix.  ./configure --prefix=$APPS_DIR/apps/zlib Para mayor informaci√≥n de como configurar el path de instalaci√≥n, es recomendable seguir la documentaci√≥n del software a instalar.\n Compilamos el c√≥digo fuente.  make  Instalamos el software.  make install # copia los binarios a --prefix  Si nos movemos al directorio donde se encuentra la instalaci√≥n, encontraremos por lo general una lista de directorios como la siguiente.  cd $APPS_DIR/apps/zlib ls # salida del comando anterior include lib share  En esos directorios se encuentran los binarios con los cuales podremos hacer uso del software.  "},{"id":16,"href":"/guia_de_usuario/jobs_schedulling/","title":"Jobs Schedulling","parent":"Gu√≠a de Usuario","content":"    Introducci√≥n      Enviar Jobs      Monitorear Recursos      Ejemplos      Introductorios      MPI y OpenMP      Python      Conda        "},{"id":17,"href":"/overview/galeria/","title":"Galer√≠a","parent":"Informaci√≥n","content":"     Vista frontal del cluster HPC Khipu        Nodos del cluster HPC Khipu        Racks disponibles del cluster HPC Khipu        Vista de todos los nodos del cluster HPC Khipu        Vista general del cluster HPC Khipu en UTEC    "},{"id":18,"href":"/guia_de_usuario/jobs_schedulling/intro/","title":"Introducci√≥n","parent":"Jobs Schedulling","content":"    Slurm Ciclo de Vida de un Job Tipos de Jobs  Batch Jobs Jobs Interactivos        Dado que Khipu es un recurso compartido, hacemos uso de un gestor de recursos como Slurm que nos permitir√° enviar, cancelar y revisar cargas de trabajo o jobs.\nSlurm     Simple Linux Utility of Resource Mangement (Slurm), es el encargado de coordinar los recursos de todos los nodos del cluster y asignarlos de acuerdo la prioridad de los jobs, cantidad de recursos solicitados y cantidad de recursos disponibles. Slurm tiene un reparto de prioridad justo, donde cada job tiene una prioridad que depende de: a) los recursos usados por el usuario o grupo, b) la contribuci√≥n del grupo al cl√∫ster y c) el tiempo en fila. Por favor, revisar grupos en Khipu para m√°s detalles de recursos.\nCiclo de Vida de un Job     Enviar un job involucra especificar los recursos necesarios y la secuencia de comandos que deseamos ejecutar en los nodos de c√≥mputo. Los pedidos se realizan a grupos de nodos de c√≥mputo llamadas particiones. Las particiones, posen sus l√≠mites y prop√≥sitos de uso. En Khipu tenemos particiones especificas para cada grupo de usuarios. Una vez enviados, los jobs esperan en una cola y est√°n sujetos a factores extra como sus prioridades de schedulling. Cuando el proceso de schedulling de un job inicia, los comandos o aplicaciones especificados son ejecutados en los nodos de computo que el scheduller asigna para satisfacer las necesidades de recurso solicitadas. El resultado de la ejecuci√≥n ser√° impreso en pantalla o escrito en un archivo dependiendo de la forma como se env√≠o el job.\nTipos de Jobs     En Slurm podemos diferenciar los siguientes tipos de jobs: batch jobs y jobs interactivos.\nBatch Jobs     Son aquellos jobs que se env√≠an a trav√©s de un bash script y que son ejecutada de manera no-interactiva (no existe interacci√≥n con los I/O). Los scripts de envi√≥ se conforman de tres partes:\n Un primera linea hashbang la cual especifica el programa que va a ejecutar el script. Por lo general es #!/bin/bash. Una lista de directivas con los requisitos del job. Estas lineas tienen que estar antes de cualquier comando o definici√≥n de variables de ambiente, caso contrario ser√°n ignoradas. Los comandos o aplicaciones que se van ejecutar en el job.  A continuaci√≥n se muestra un ejemplo b√°sico de un script.\n#!/bin/bash  ## Directivas # Job name: #SBATCH --job-name=simple_example  # Nombre del archivo de salida: #SBATCH --out=\u0026#34;slurm-%j.out\u0026#34; # Tiempo limite de ejecuci√≥n: #SBATCH --time=01:00 # Cantidad de recursos #SBATCH --nodes=1 --ntasks=1 --cpus-per-task=2  ## Secuencia de comandos echo \u0026#34;Iniciando en $(date)\u0026#34; echo \u0026#34;El job fue enviado a la partici√≥n ${SLURM_JOB_PARTITION}, la partici√≥n por defecto es ${SLURM_CLUSTER_NAME}\u0026#34; echo \u0026#34;Nombre del job: ${SLURM_JOB_NAME}, Job ID: ${SLURM_JOB_ID}\u0026#34; echo \u0026#34;Tengo ${SLURM_CPUS_ON_NODE}CPUs en el nodo $(hostname)\u0026#34; echo \u0026#34;Voy a dormir 10s para que me veas en la fila\u0026#34; sleep 10 Para probarlo, deberemos guardarlo con el siguiente formato ej-simple.sh y lo mandaremos ejecutando.\nsbatch ej-simple.sh Ejemplos m√°s detallados de bash scripts se muestran aqu√≠.\nJobs Interactivos     Los jobs interactivos son aquellos que pueden ser usados para testear y encontrar errores en un c√≥digo. Al pedir un job interactivo, se reservaran los recursos y se iniciar√° sesi√≥n en un shell de alguno de los nodos de c√≥mputo. A continuaci√≥n se muestra un ejemplo de job interactivo:\nsrun --pty -t 2:00:00 --mem=8G -p docencia bash El comando anterior asignar√° un CPU y 8GiB RAM por un periodo de 2 horas. Durante ese tiempo podremos ejecutar comandos dentro de la shell de manera interactiva. Para salir de la sesi√≥n, deberemos ejecutar `exit` o presionar `Ctrl`+`d`. "},{"id":19,"href":"/guia_de_usuario/jobs_schedulling/monitorear_recursos/","title":"Monitorear Recursos","parent":"Jobs Schedulling","content":"    Recomendaci√≥n General üí°  Jobs en ejecuci√≥n Jobs finalizados  seff sacct          Recomendaci√≥n General üí°     Aseg√∫rese de reservar la cantidad de RAM y CPUs necesarios para la ejecuci√≥n de su job. No reserve recursos que no necesita, ya que de hacerlo, perjudicar√° la ejecuci√≥n de los dem√°s usuarios del cluster.\nA continuaci√≥n, se muestran algunos ejemplos de como medir el uso de CPU y RAM de su job a fin de que pueda refinar la reserva de recursos. No olvide revisar la documentaci√≥n sobre el env√≠o de jobs a Slurm.\nJobs en ejecuci√≥n     Si su job se encuentra en ejecuci√≥n, usted puede revisar su uso actual de recursos. Sin embargo, deber√° esperar hasta su finalizaci√≥n para ver el uso m√°ximo de recursos durante toda su ejecuci√≥n.\nLa manera m√°s sencilla de revisar el uso instant√°neo de recursos es hacer ssh al nodo de computaci√≥n donde su job se encuentra ejecut√°ndose. Para saber a que nodo debe hacer ssh, ejecute:\n[juan.perez@khipu ~]$ squeue --me JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 21615 investiga bert-sar juan PD 0:00 1 n001 Notamos que nuestro job bert-sar se encuentra ejecut√°ndose en el valor de la columna NODELIST y nos conectamos usando ssh.\n[juan.perez@khipu ~]$ ssh n001 Una vez dentro del nodo de c√≥mputo, ejecutaremos ps o htop.\n  ps le brindar√° la informaci√≥n instant√°nea del uso de recursos cada vez que ejecute el comando.\n[juan.perezo@n004 ~]$ ps -u$USER -o %cpu,rss,args %CPU RSS COMMAND 0.0 2376 python triangle.py 0.0 2380 python triangle.py 0.0 2380 python triangle.py 0.0 2380 python triangle.py 0.0 2380 python triangle.py 0.0 2380 python triangle.py 0.0 2380 python triangle.py 0.0 2380 python triangle.py 0.0 2380 python triangle.py El reporte de memoria de ps se muestra en KB, podemos notar que los procesos listados consumen alrededor de 2000 KB de RAM y que el uso de los CPUs es casi nulo.\n  htop se ejecuta de manera interactiva y muestra las estad√≠sticas de uso en vivo. Puede presionar la tecla u, ingresar su nombre de usuario y luego enter para filtrar solo sus procesos. La informaci√≥n del uso de memoria, se encuentra en la columna RES. Para solicitar ayuda puede presionar ? y si desea salir q .\n  Jobs finalizados     Slurm guarda las estad√≠sticas de cada job, inclu√≠do cuanta memoria y CPU fue utilizada.\nseff      Luego de finalizado un job, puede ejecutar seff\u0026lt;jobid\u0026gt; para obtener informaci√≥n √∫til sobre la ejecuci√≥n de su job, tal como la memoria usada y a que porcentaje de la memoria reserva corresponde.  [juan.perez@khipu ~]$ seff 21886 Job ID: 21886 Cluster: cluster Use of uninitialized value $group in concatenation (.) or string at /usr/bin/seff line 154, \u0026lt;DATA\u0026gt; line 602. User/Group: juan.perez/ State: COMPLETED (exit code 0) Cores: 1 CPU Utilized: 00:00:00 CPU Efficiency: 0.00% of 00:00:00 core-walltime Job Wall-clock time: 00:00:00 Memory Utilized: 0.00 MB (estimated maximum) Memory Efficiency: 0.00% of 50.00 GB (50.00 GB/core) sacct     Tambi√©n se puede usar sacct para obtener la informaci√≥n del job. Lamentablemente, el output por defecto de sacct no es del todo entendible, por ello se recomienda procesar la salida de la siguiente manera.\n[juan.perez@khipu ~]$ export SACCT_FORMAT=\u0026quot;JobID%20,JobName,User,Partition,NodeList,Elapsed,State,ExitCode,MaxRSS,AllocTRES%32\u0026quot; [juan.perez@khipu ~]$ sacct -j 21886 JobID JobName User Partition NodeList Elapsed State ExitCode MaxRSS AllocTRES -------------------- ---------- --------- ---------- --------------- ---------- ---------- -------- ---------- 21886 125000000+ juan.pe+ investiga+ g001 00:00:00 COMPLETED 0:0 billing=1,cpu=1,mem=50G,node=1 21886.batch batch g001 00:00:00 COMPLETED 0:0 cpu=1,mem=50G,node=1 "},{"id":20,"href":"/guia_de_usuario/jobs_schedulling/recursos_a_usar/","title":"¬øQu√© recursos pueden usarse?","parent":"Jobs Schedulling","content":"https://docs.ycrc.yale.edu/clusters-at-yale/job-scheduling/resource-requests/\n"},{"id":21,"href":"/guia_de_usuario/jobs_schedulling/enviar_jobs/","title":"Enviar Jobs","parent":"Jobs Schedulling","content":"  Comandos b√°sicos de Slurm  Job request m√°s comunes      Comandos b√°sicos de Slurm     A continuaci√≥n se muestra una lista con los comandos de Slurm m√°s usados:\n Para enviar un batch job a la cola de slurm ejecutaremos sbatch.  sbatch ejemplo.sh  Para listar los jobs que he enviado a la cola usaremos squeue.  squeue --me  Para cancelar un job en ejecuci√≥n usando su job ID emplearemos scancel.  scancel 78910  Para revisar el estatus de un job usaremos sacct y le pasaremos un job ID.  sacct -j 78910  Para revisar cuan eficientemente un job se ejecuta, emplearemos seff acompa√±ado del job ID.  seff 78910  Para ejecutar un job de manera interactiva emplearemos srun  srun --pty -t 2:00:00 --mem=8G -p interactive bash Job request m√°s comunes     Las siguientes opciones modifican el tama√±o, largo y el comportamiento del job que se env√≠a. Estos pueden especificarse llamando a srun o sbatch, o dentro de un batch job. Si se especifican las opciones en los argumentos de sbatch y en el script del batch job al mismo tiempo, las opciones pasadas al comando sbatch ser√°n las que se tomar√°n en cuenta. Si no se especifica valor para alguna de las opciones, los valores por defecto ser√°n los que se empleen.\n .myTable { border-radius: 5px; } .myTable th { background-color:var(--body-font-color); color: white; }     Opci√≥n Larga Opci√≥n Corta Valor por Defecto Descripci√≥n     --job-name -J Nombre del archivo Nombre de job personalizado.   --output -o \u0026quot;slurm-%j.out\u0026quot; Nombre del archivo donde se guadar√° la salida stdout o stderr. Mayores patrones de nombre aqu√≠.   --error -e Se escribe en el mismo archivo del --output Nombre del archivo donde se guadar√°n los logs de ;ps errores.   --partition -p Var√≠a de acuerdo al cluster Se√±ala la partici√≥n donde se va a ejecutar el job.   --account -A El nombre de su grupo Especifica si se tiene acceso a m√∫ltiples particiones privadas.   --time -t Var√≠a de acuerda a la partici√≥n L√≠mite de tiempo para el job en el formato D-HH:MM:SS. Por ejemplo, -t 1- es un d√≠a de ejecuci√≥n y -t 4:00:00 son 4 horas.   --nodes -N 1 N√∫mero total de nodos.   --ntasks -n 1 N√∫mero de tareas (workers MPI).   --ntasks-per-node  El scheduler lo decide N√∫mero de tareas por nodo.   --cpus-per-task -c 1 N√∫mero de CPUs para cada tarea. Use esto para threads/cores en un job de nodo √∫nico.   --mem-per-cpu  5G Cantidad de memoria RAM requerida por CPU en MiB. Si se especifica en GiB usar G(ej. 10GB).   --mem   Memoria pedida por nodo en MiB. Si se especifica en GiB usar G(ej. 10GB).   --gpus G  Usado para pedir GPUs.   --constraint C  Restricciones a las caracter√≠sticas del nodo. Para limitar los tipos de nodos a ejecutarse.   --mail-user  Tu email de UTEC Direcci√≥n de correo a donde enviar notificaciones del job.   --mail-type  Ninguna Env√≠a un mail cada vez que un job cambia de estado. Utilice la opci√≥n ALL para recibir notificaciones al iniciar y terminar un job. Opciones disponibles ALL, BEGIN, END, FAIL, NONE    "},{"id":22,"href":"/guia_de_usuario/jobs_schedulling/ejemplos/introductorios/","title":"Introductorios","parent":"Ejemplos","content":"    Imprimir la fecha actual Imprimir las variables de ambiente de Slurm      Imprimir la fecha actual      Crear el archivo fecha_actual.sh:  #!/bin/bash  # Nombre del job: #SBATCH --job-name=fecha_actual  # Cantidad de CPUs cores a usar: #SBATCH -c 1 # Tama√±o de memoria del job: #SBATCH --mem-per-cpu=100mb date sleep 10 # duerme 10s, solo para visualizar el job en la fila Enviar a ejecutar el job:  sbatch fecha_actual.sh Imprimir las variables de ambiente de Slurm      Crear el archivo env_vars.sh:  #!/bin/bash  # Nombre del job: #SBATCH --job-name=env_vars  # Comandos: set | grep SLURM Enviar a ejecutar el job:  sbatch env_vars.sh "},{"id":23,"href":"/guia_de_usuario/jobs_schedulling/ejemplos/mpi_openmp/","title":"MPI y OpenMP","parent":"Ejemplos","content":"    OpenMP de un solo thread OpenMP de m√∫ltiples threads MPI multi-proceso MPI Y OpenMP a la vez (H√≠brido)      Para los siguientes dos ejemplos con OpenMP usaremos el siguiente c√≥digo en C++ prueba_openmp.cpp.\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;omp.h\u0026gt; void Hello(void); /* Thread function */ /*--------------------------------------------------------------------*/ int main(int argc, char* argv[]) { int thread_count = strtol(argv[1], NULL, 10); # pragma omp parallel num_threads(thread_count)  Hello(); return 0; } /* main */ /*------------------------------------------------------------------- * * Function: Hello * * Purpose: Thread function that prints message * */ void Hello(void) { int my_rank = omp_get_thread_num(); int thread_count = omp_get_num_threads(); printf(\u0026#34;Hello from thread %d of %d\\n\u0026#34;, my_rank, thread_count); /* Hello */ } Compilaremos el programa antes de crear y enviar el batch script.\nmodule load gcc/5.5.0 g++ prueba_openmp.cpp -fopenmp -lpthread -o prueba_openmp module unload gcc/5.5.0 OpenMP de un solo thread      Crear el archivo single_thread_openmp.sh:  #!/bin/bash  # Nombre del job: #SBATCH --job-name=single_thread_openmp # L√≠mite de tiempo de 10 min: #SBATCH --time=10:00 ./prueba_openmp Enviar a ejecutar el job:  sbatch single_thread_openmp.sh OpenMP de m√∫ltiples threads      Crear el archivo multi_thread_openmp.sh:  #!/bin/bash  # Nombre del job: #SBATCH --job-name=multi_thread_omp_job # Nombre del archivo de salida: #SBATCH --output=multi_thread_omp_job.txt # Numero de tasks: #SBATCH --ntasks=1 # Numero de CPUs por task: #SBATCH --cpus-per-task=4 # L√≠mite de tiempo de 10 min: #SBATCH --time=10:00 export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK ./prueba_openmp Enviar a ejecutar el job:  sbatch multi_thread_openmp.sh MPI multi-proceso     Para el siguiente ejemplo usaremos el c√≥digo prueba_mpi.c:\n#include \u0026lt;mpi.h\u0026gt;#include \u0026lt;stdio.h\u0026gt; int main(int argc, char** argv) { // Initialize the MPI environment MPI_Init(NULL, NULL); // Get the number of processes int world_size; MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;world_size); // Get the rank of the process int world_rank; MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;world_rank); // Get the name of the processor char processor_name[MPI_MAX_PROCESSOR_NAME]; int name_len; MPI_Get_processor_name(processor_name, \u0026amp;name_len); // Print off a hello world message printf(\u0026#34;Hello world from processor %s, rank %d out of %d processors\\n\u0026#34;, processor_name, world_rank, world_size); // Finalize the MPI environment. MPI_Finalize(); }  Compilar el programa mpi:  module load mpich/4.0 mpicc prueba_mpi.c -o prueba_mpi module unload mpich/4.0 Crear el archivo multi_process_mpi.sh:  #!/bin/bash  # Nombre del job: #SBATCH -J prueba_mpi # Nombre de la partici√≥n: #SBATCH -p investigacion # N√∫mero de nodos: #SBATCH -N 2  # N√∫mero de tasks por nodo: #SBATCH --tasks-per-node=3  # Carga del modulo MPICH 4.0 module load mpich/4.0 # Ejecuci√≥n del compilado mpirun prueba_mpi # Descarga del m√≥dulo module unload mpich/4.0 Enviar a ejecutar el job:  sbatch multi_process_mpi.sh MPI Y OpenMP a la vez (H√≠brido)     Para este ejemplo usaremos el siguiente codigo hibrido y lo guardaremos en un archivo hibrido.c.\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;omp.h\u0026gt;#include \u0026#34;mpi.h\u0026#34; int main(int argc, char *argv[]) { int numprocs, rank, namelen; char processor_name[MPI_MAX_PROCESSOR_NAME]; int iam = 0, np = 1; MPI_Init(\u0026amp;argc, \u0026amp;argv); MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;numprocs); MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;rank); MPI_Get_processor_name(processor_name, \u0026amp;namelen); #pragma omp parallel default(shared) private(iam, np)  { np = omp_get_num_threads(); iam = omp_get_thread_num(); printf(\u0026#34;Hello from thread %d out of %d from process %d out of %d on %s\\n\u0026#34;, iam, np, rank, numprocs, processor_name); } MPI_Finalize(); }  Compilar el programa h√≠brido:  module load mpich/4.0 mpicc -fopenmp hibrido.c -o hibrido module unload mpich/4.0 Crear el archivo hibrido_mpi_openmp.sh:  #!/bin/bash  # Un Job script para la ejecuci√≥n de un c√≥digo h√≠brido MPI-OpenMP #SBATCH --job-name=hibrido_mpi_openmp #SBATCH --output=hibrido_mpi_openmp.out #SBATCH --ntasks=4 #SBATCH --cpus-per-task=8 #SBATCH --partition=docencia # Cargar el modulo MPI. module load mpich/4.0 # Configurar el valor de OMP_NUM_THREADS con el numero de CPUs por task solicitado. export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK # Ejecutar el proceso con mpirun. Puede notar que no es necesario especificar el flag de MPI # \u0026#39;-n\u0026#39; puesto que automaticamente utiliza el valor de la configuraci√≥n de Slurm realizada mpirun ./hibrido module unload mpich/4.0 Enviar a ejecutar el job:  sbatch hibrido_mpi_openmp.sh "},{"id":24,"href":"/guia_de_usuario/jobs_schedulling/ejemplos/python/","title":"Python","parent":"Ejemplos","content":"Ejecuci√≥n del programa en Python, prueba_python.py:\nfrom math import factorial as f print(\u0026#34;Hola mundo\u0026#34;) N = 100 print(\u0026#34;%d! = %d\u0026#34; %(N, f(N)))  Crear el archivo ej5.sh:  #!/bin/bash #SBATCH -J ej5 # nombre del job #SBATCH -p investigacion # nombre de la particion  #SBATCH -c 1 # numero de cpu cores a usar module load python/2.7.17 # carga el modulo de python version 2.7.17 python2.7 prueba_python.py # siendo prueba_python.py el nombre del programa python module unload python/2.7.17 Enviar a ejecutar el job:  sbatch ej5.sh "},{"id":25,"href":"/guia_de_usuario/jobs_schedulling/ejemplos/conda/","title":"Conda","parent":"Ejemplos","content":"En Khipu, se puede emplear el gestor de paquetes conda a trav√©s del m√≥dulo Miniconda. Para ello ser√° necesario necesario ejecutar los siguiente pasos:\nCreaci√≥n de ambiente de trabajo       Desde el nodo de acceso, cargaremos el modulo de Miniconda.\nmodule load miniconda/3.0   Crearemos el ambiente de conda sobre el cual trabajaremos.\nconda create --name my-env   Si deseamaos crear un ambiente y a la vez instalar paquetes.\nconda create --name my-env pytorch torchvision   Creaci√≥n de batch script     En el script que creemos, deberemos especificar la cantidad de recursos que necesitamos. Luego deberemos cargar el m√≥dulo de Miniconda y activar nuestro ambiente de trabajo. Finalmente, escribiremos el comando necesario para ejecutar nuestro programa.\n#!/bin/bash #SBATCH --job-name=app-test # nombre del job #SBATCH --nodes=1 # cantidad de nodos #SBATCH --ntasks=1 # cantidad de tareas #SBATCH --cpus-per-task=1 # cpu-cores por task  #SBATCH --mem=4G # memoria total por nodo #SBATCH --gres=gpu:1 # numero de gpus por nodo #SBATCH --time=00:05:00 # limite total de ejecucion module purge module load miniconda/3.0 conda activate my-env python app-test.py conda deactivate Si al momento de enviar su job obtienen el siguiente mensaje.\nCommandNotFoundError: Your shell has not been properly configured to use \u0026#39;conda activate\u0026#39;. To initialize your shell, run $ conda init \u0026lt;SHELL_NAME\u0026gt; Currently supported shells are: - bash - fish - tcsh - xonsh - zsh - powershell See \u0026#39;conda init --help\u0026#39; for more information and options. IMPORTANT: You may need to close and restart your shell after running \u0026#39;conda init\u0026#39;. Deberan adicionar eval \u0026quot;$(conda shell.bash hook)\u0026quot; a su batch script luego de haber cargado el modulo de miniconda. Por lo tanto, el batch script resultante ser√° el siguiente.\n#!/bin/bash #SBATCH --job-name=app-test # nombre del job #SBATCH --nodes=1 # cantidad de nodos #SBATCH --ntasks=1 # cantidad de tareas #SBATCH --cpus-per-task=1 # cpu-cores por task  #SBATCH --mem=4G # memoria total por nodo #SBATCH --gres=gpu:1 # numero de gpus por nodo #SBATCH --time=00:05:00 # limite total de ejecucion module purge module load miniconda/3.0 eval \u0026#34;$(conda shell.bash hook)\u0026#34; conda activate my-env python app-test.py conda deactivate "},{"id":26,"href":"/guia_de_usuario/jobs_schedulling/ejemplos/","title":"Ejemplos","parent":"Jobs Schedulling","content":"    Introductorios      MPI y OpenMP      Python      Conda      "},{"id":27,"href":"/guia_de_usuario/transfiriendo_data/","title":"Transfiriendo Data","parent":"Gu√≠a de Usuario","content":"    Transferir archivos usando scp Transferir archivos usando rsync Revisar la cantidad de almacenamiento usado      Se recomienda realizar regularmente copias de sus archivos en Khipu a sus ordenadores locales. Es importante recordar que Khipu no puede ser usado como almacenamiento personal en la nube. Por otro lado, se recomienda comprimir previamente los archivos que se van a transferir a fin de agilizar el proceso.\nTransferir archivos usando scp     ¬øQu√© es \u0026#39;scp\u0026#39;? ‚Üï  Secure Copy Protocol (scp)     scp es un comando que permite copiar de manera secura archivos y directorios desde dos ubicaciones remotas. Con este comando podremos copiar archivos o directorios desde nuestro ordenador local a uno remoto y viceversa. Cuando se transfiere datos mediante scp los archivos y contrase√±as son encriptados para evitar que alguien tenga acceso no autorizado mientras se realiza el proceso de copia.\nA continuaci√≥n se muestran diversos ejemplos de como usar el comando scp.\n    Copiar desde mi ordenador local a Khipu\n# Copiar my-archivo.txt a mi directorio /home en Khipu scp my-archivo.txt my-usuario@khipu.utec.edu.pe:~ # Copiar my-folder/ a mi directorio /home en Khipu. # El flag -r indica que se esta ejecutando el comando de manera recursiva. scp -r my-folder my-usuario@khipu.utec.edu.pe:~   Copiar de Khipu a mi ordenador local\n# Copiar my-archivo.txt a mi directorio actual en mi ordenador scp my-usuario@khipu.utec.edu.pe:~/path/to/my-archivo.txt . # Copiar my-folder/ a mi directorio actual en mi ordenador scp -r my-usuario@khipu.utec.edu.pe:~/path/to/my-folder .    Mayor informaci√≥n: https://linux.die.net/man/1/scp\n Transferir archivos usando rsync     ¬øQu√© es \u0026#39;rsync\u0026#39;? ‚Üï  Remote Synchronization (rsync)     rsync (Remote Synchronization) es una herramienta de sincronizaci√≥n entre archivos remotos y locales. Este comando minimiza la cantidad de datos copiados, ya que solo copia aquellas partes que cambiaron entre ambos directorios.\nA continuaci√≥n se muestran algunos ejemplos del uso de rsync.\n  # Sincronizar un directorio local a uno remoto.  # El flag -a indica que se trata de archivos rsync -a ~/path/to/my-folder my-usuario@khipu.utec.edu.pe:~/path/in/khipu # Sincronizar un directorio local a uno remoto rsync -a my-usuario@khipu.utec.edu.pe:~/path/to/my-folder ~/path/in/local # Sincronizar un directorio local a uno remoto comprimiendo previamente # El flag -az indica que se trata de archivos que se van a comprimir rsync -az ~/path/to/my-folder my-usuario@khipu.utec.edu.pe:~/path/in/khipu # Sincronizar un directorio remoto a uno local comprimiendo previamente y mostrando el progreso # El flag -azP indica que se trata de archivos que se van a comprimir y se muestra el progreso rsync -azP my-usuario@khipu.utec.edu.pe:~/path/to/my-folder ~/path/in/local  Mayor informaci√≥n: https://linux.die.net/man/1/rsync\n Revisar la cantidad de almacenamiento usado     El comando du (disk usage) es empleado para conocer el espacio que ocupa un archivo o directorio. A continuaci√≥n se muestran algunos ejemplos √∫tiles:\n# Revisar cuanto almacenamiento voy usando en mi /home cd ~; du -hs # Revisar cuanto almacenamiento voy usando por cada carpeta en mi /home  cd ~; du -d 1 -h # Revisar cuanto almacenamiento voy usando por cada carpeta en mi /home y ordenar por tama√±o  cd ~; du -d 1 -h | sort -hr  Mayor informaci√≥n: https://man7.org/linux/man-pages/man1/du.1.html\n "},{"id":28,"href":"/guia_de_usuario/configuracion_de_hardware/","title":"Configuracion de Hardware","parent":"Gu√≠a de Usuario","content":""},{"id":29,"href":"/ayuda/","title":"Ayuda","parent":"Inicio","content":"    Solicitando Ayuda     ¬øC√≥mo pedir ayuda?     Preguntas que podemos atender     Recomendaciones       FAQs     Tutoriales     "},{"id":30,"href":"/ayuda/solicitando_ayuda/","title":"Solicitando Ayuda","parent":"Ayuda","content":""},{"id":31,"href":"/ayuda/solicitando_ayuda/como_pedir_ayuda/","title":"¬øC√≥mo pedir ayuda?","parent":"Solicitando Ayuda","content":""},{"id":32,"href":"/ayuda/solicitando_ayuda/preguntas_a_atender/","title":"Preguntas que podemos atender","parent":"Solicitando Ayuda","content":""},{"id":33,"href":"/ayuda/solicitando_ayuda/recomendaciones/","title":"Recomendaciones","parent":"Solicitando Ayuda","content":""},{"id":34,"href":"/ayuda/faqs/","title":"FAQs","parent":"Ayuda","content":""},{"id":35,"href":"/ayuda/tutoriales/","title":"Tutoriales","parent":"Ayuda","content":""},{"id":36,"href":"/reportes/","title":"Reportes","parent":"Inicio","content":"Importante: Informaci√≥n actualizada cada 2 minutos.  Reporte General     Reporte de CPU      Reporte de Memoria     Reporte de Red       "},{"id":37,"href":"/","title":"Inicio","parent":"","content":"Bienvenido a la documentaci√≥n de Khipu   Esta es la documentaci√≥n oficial del cluster Khipu. En este sitio encontrar√°s toda la informaci√≥n sobre el cluster, las pol√≠ticas y gu√≠as de uso.\nIniciar   Enlaces R√°pidos   Primeros Pasos   Si no sabe por donde empezar con la documentaci√≥n, este es un buen punto de comienzo.  Gu√≠a de Usuario   Si desea informaci√≥n detallada sobre el uso y manejo del cluster, puede consultar aqu√≠.  Reportes   Informaci√≥n actualizada cada dos minutos sobre el estado actual del cluster est√° aqu√≠.   Anuncios   La informaci√≥n actualizada m√°s relevante correspondiente al funcionamiento del cluster est√° aqu√≠.   Zero initial configuration   Getting started in minutes. The theme is shipped with a default configuration and works out of the box.  Handy shortcodes   We included some (hopefully) useful custom shortcodes so you don\u0026rsquo;t have to and can focus on writing amazing docs.  Dark mode   Powerful dark mode that detects your system preferences or can be controlled by a toggle switch.   Sobre Khipu   Khipu es un cluster dedicado a la computaci√≥n de alto desempe√±o, en ingl√©s High performance Computing (HPC). Est√° formado por una colecci√≥n de servidores distribu√≠dos, llamados nodos que se encuentran conectados a trav√©s de una interconexi√≥n de alta velocidad (Infiniband).\nKhipu es parte del Centro de Investigaci√≥n en Computaci√≥n Sostenible (COMPSUST) de la Universidad de Ingenier√≠a y Tecnolog√≠a (UTEC).\n Contacto: khipu@utec.edu.pe.\nPara informaci√≥n de acceso revisar aqu√≠.\n Grupos de usuarios   Existen dos grupos de usuarios gerenciados autom√°ticamente por Slurm: 1) investigaci√≥n, y 2) educaci√≥n. Ambos grupos acceden al cluster por el nodo l√≠der para enviar trabajos a la fila de ejecuci√≥n.\nGrupo Educaci√≥n   Permite el uso del cluster para aula o laboratorio por los estudiantes a pedido de un instructor(a) registrado(a). Este grupo es financiado por la universidad y sus trabajos ser√°n procesados bajo las siguientes caracter√≠sticas:\n Ocupar un nodo CPU y/o GPU. Equivalente a un total de 72 cores, 320 GB de memoria y 1.4 TB de almacenamiento.\n Tiempo m√°ximo de ejecuci√≥n de trabajo: 2 horas.\n N√∫mero de procesos concurrentes: 2.\n Cantidad de cores por proceso 8.  Grupo Investigaci√≥n   Este grupo est√° financiado por UTEC, proyectos de investigaci√≥n (PI), Departamentos y Direcci√≥n de Escuela. Los fondos centrales cubren costos de infraestructura, operaci√≥n y soporte. Los PI y algunas unidades y departamentos financian la adquisici√≥n de nuevos nodos de procesamiento y almacenamiento. Los usuarios de este grupo ejecutan trabajos en todos los nodos bajo las siguientes caracter√≠sticas:\n Ocupar cualquier nodo CPU y/o GPU. Equivalente a un total de 200 cores, 1.1 TB de memoria y 5.3 TB de almacenamiento.\n Tiempo m√°ximo de ejecuci√≥n de trabajo NaN.\n N√∫mero de procesos concurrentes NaN.\n Cantidad de cores por proceso NaN.   Infraestructura   Para mayor informaci√≥n sobre programas, ver Software\n .myTable { border-radius: 5px; } .myTable th { background-color:var(--header-background); color: white; }     nL√≠der     Nodo de acceso al cluster, usado principalmente para compilar y enviar trabajos.   Procesadores: Intel(R) Xeon(R) Gold 6230 CPU @ 2.10 GHz 20 cores por socket, 40 por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 480 SSD, total 40 TB HDD   Network: Infiniband FDR MT4119       nCPU (1, 2, 3, 4, 5)     Usado para procesamiento, gerenciado autom√°ticamente por Slurm.   Procesadores: Intel(R) Xeon(R) Gold 6130 CPU @2.10 GHz 16 cores por socket, 32 por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 960 GB SSD   Network: Infiniband FDR MT4119       nGPU 1     Usado para procesamiento, gerenciado autom√°ticamente por Slurm.   Procesadores: Intel(R) Xeon(R) Gold 6230 CPU @2.10 GHz 20 cores por socket, 40 por nodo.   Gr√°ficos: NVIDIA Tesla T4 16 GB GDDR6, PCIe 3.0 x16 1 GPU por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 480 GB SSD   Network: Infiniband FDR MT4119       nGPU 2     Usado para procesamiento, gerenciado autom√°ticamente por Slurm.   Procesadores: AMD EPYC 7742 @2.24 GHz 64 cores por socket, 128 por nodo.   Gr√°ficos: NVIDIA Tesla T4 16 GB GDDR6, PCIe 3.0 x16 1 GPU por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 480 GB SSD   Network: Infiniband FDR MT4119     --   "},{"id":38,"href":"/tags/","title":"Tags","parent":"Inicio","content":""}]