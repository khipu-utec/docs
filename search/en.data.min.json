[{"id":0,"href":"/","title":"Khipu","parent":"","content":"Sobre Khipu   Khipu es un cluster dedicado a la computación de alto desempeño, en inglés High performance Computing (HPC). Está formado por una colección de servidores distribuídos, llamados nodos. Cada nodo está conectados a través de una interconexión de alta velocidad (Infiniband), y tiene CPUs y/o GPUs, memoria y almacenamiento.\nKhipu es parte del Centro de Investigación en Computación Sostenible (COMPSUST) de la Universidad de Ingeniería y Tecnología (UTEC).\n Contacto: khipu@utec.edu.pe.\nPara información de acceso ver Política de uso.\n Grupos de usuarios   Existen dos grupos de usuarios gerenciados automáticamente por Slurm: 1) investigación, y 2) educación. Ambos grupos acceden al cluster por el nodo líder para enviar trabajos a la fila de ejecución.\nGrupo Educación   Permite el uso del cluster para aula o laboratorio por los estudiantes a pedido de un instructor(a) registrado(a). Este grupo es financiado por la universidad y sus trabajos serán procesados bajo las siguientes características:\n Ocupar un nodo CPU y/o GPU. Equivalente a un total de 72 cores, 320 GB de memoria y 1.4 TB de almacenamiento.\n Tiempo máximo de ejecución de trabajo 2 horas.\n Número de procesos concurrentes 2.\n Cantidad de cores por proceso 8.  Grupo Investigación   Este grupo está financiado por UTEC, proyectos de investigación (PI), Departamentos y Dirección de Escuela. Los fondos centrales cubren costos de infraestructura, operacion y soporte. Los PI y algunas unidades y departamentos financian la adquisición de nuevos nodos de procesamiento y almacenamiento. Los usuarios de este grupo ejecutan trabajos en todos los nodos bajo las siguientes características:\n Ocupar cualquier nodo CPU y/o GPU. Equivalente a un total de 200 cores, 1.1 TB de memoria y 5.3 TB de almacenamiento.\n Tiempo máximo de ejecución de trabajo NaN.\n Número de procesos concurrentes NaN.\n Cantidad de cores por proceso NaN.   Infraestructura   Para mayor información sobre programas, ver Software\n .myTable { border-radius: 5px; } .myTable th { background-color:var(--header-background); color: white; }     nLíder     Nodo de acceso al cluster, usado compilar y principalmente para enviar trabajos.   Procesadores: Intel(R) Xeon(R) Gold 6230 CPU @ 2.10 GHz 20 cores por socket, 40 por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 480 SSD, total 40 TB HDD   Network: Infiniband FDR MT4119       nCPU     Usado para procesamiento, gerenciado automáticamente por Slurm.   Procesadores: Intel(R) Xeon(R) Gold 6130 CPU @2.10 GHz16 cores por socket, 32 por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 960 GB SSD   Network: Infiniband FDR MT4119       nGPU     Usado para procesamiento, gerenciado automáticamente por Slurm.   Procesadores: Intel(R) Xeon(R) Gold 6230 CPU @2.10 GHz 20 cores por socket, 40 por nodo.   Gráficos: NVIDIA Tesla T4 16 GB GDDR6, PCIe 3.0 x16 1 GPU por nodo.   Memoria: DRAM DDR4-1333 MHz, 128 GB por nodo   Disco local: 480 GB SSD   Network: Infiniband FDR MT4119       "},{"id":1,"href":"/politica/","title":"Política","parent":"Khipu","content":""},{"id":2,"href":"/uso/","title":"Uso","parent":"Khipu","content":""},{"id":3,"href":"/anuncios/","title":"Anuncios","parent":"Khipu","content":""},{"id":4,"href":"/uso/slurm/","title":"Slurm","parent":"Uso","content":"Última actualización: 23 Agosto 2020.     Sobre Slurm  Directivas de jobs Ejemplos de job scripts Ejemplos de job scripts con MPI, OpenMP y Python Ejemplos de job scripts avanzados Información adicional        Sobre Slurm     Slurm es un gestor de filas de trabajos (jobs) que permite organizar de forma eficiente el uso de recursos compartido del cluster. Slurm asigna recursos a los jobs de acuerdo con la prioridad del trabajo y la cantidad de recursos solicitados versus disponibles. Slurm tiene un reparto de prioridad justo, donde cada job tiene una prioridad que depende de: a) los recursos usados por el usuario o grupo, b) la contribución del grupo al clúster y c) el tiempo en fila. Por favor, revisar grupos en Khipu para más detalles de recursos. Los comandos más usados son:\n sbatch: permite adicionar jobs a la fila mediante un script. Link del manual de sbatch. srun: permite adicionar jobs en modo interactivo lo que permite probar comandos que se usarán en el script así como depurarlo. Link del manual de srun. scancel: termina la ejecución de un determinado job, independientemente si está o no en ejecución. Link del manual de scancel. squeue: permite visualizar el estado actual de la fila de jobs. Link del manual de squeue.   Para mandar a ejecutar un job usando el script de ejemplo ej1.sh.  Mostrar ej1.sh ↕  #!/bin/bash #SBATCH --job-name=ej1 # nombre del job #SBATCH -c 1 # numero de cpu cores a usar #SBATCH --mem-per-cpu=100mb # tamano de memoria del job en ejecucion. date sleep 10 # duerme 10s, solo para visualizar el job en la fila    [juana.perez@khipu ~]$ sbatch ej1.sh Submitted batch job 91 Para mostrar la fila actual de jobs:  [juana.perez@khipu ~]$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 91 investigacion EJ2 juana.perez R 0:29 1 n001 Para mostrar todos los detalles del job 91:  [juana.perez@khipu ~]$ scontrol show job 91 Para cancelar la ejecución del job 91:  [juana.perez@khipu ~]$ scancel 91 Directivas de jobs     Importante: verificar los recursos de Khipu. Slurm regulará automáticamente en caso el usuario exceda las capacidades del Grupo de Usuario.     Directiva Versión corta Descripción     --mail-user=\u0026lt;email_address\u0026gt;  Correo electrónico   after:job_id[[+time][:jobid[+time]...]]  El job se mandará a la fila luego que otro job termine su ejecución.   --mail-type=\u0026lt;ALL,BEGIN,END,FAIL,NONE\u0026gt;  Enviar notificaciones de job a email   --gres=gpu:\u0026lt;cantidadGPU\u0026gt;  Especifica la cantidad de tarjetas GPU que se necesitan.   --export=var1=valor[,var2=valor...]  Especifica los valores de las variables de entorno en la ejecución del script.   --nodes=\u0026lt;nodes\u0026gt;  -N \u0026lt;nodes\u0026gt; Nodos requeridos por job.   --job-name=\u0026lt;Nombrejob \u0026gt;  -J \u0026lt;Nombrejob\u0026gt;  Nombre del job.   --cpus-per-task=\u0026lt;cpus\u0026gt;  -c \u0026lt;cpus\u0026gt; Núcleos CPU por tarea.   --nodes=\u0026lt;numeroNodos\u0026gt;  -N \u0026lt;numeroNodos\u0026gt; Número de nodos solicitados para la ejecución de los jobs.   --output= \u0026lt;rutaArchivo.log \u0026gt;  -o \u0026lt;rutaArchivo.log\u0026gt; Ruta de archivo para almacenar la salida estandar del job.   --error = \u0026lt;rutaArchivo.log \u0026gt;  -e \u0026lt;rutaArchivo.log\u0026gt; Ruta de archivo para almacenar errores en jobs.   --time=\u0026lt;tiempo\u0026gt;  -t \u0026lt;tiempo\u0026gt; Tiempo requerido.    Ejemplos de job scripts     Ejemplo 1  Imprime la fecha actual.\n Crear el archivo ej1.sh:  #!/bin/bash #SBATCH --job-name=ej1 # nombre del job #SBATCH -c 1 # numero de cpu cores a usar #SBATCH --mem-per-cpu=100mb # tamano de memoria del job en ejecucion. date sleep 10 # duerme 10s, solo para visualizar el job en la fila Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej1.sh   Ejemplo 2  Imprime \u0026ldquo;Hello World”.\n Crear el archivo ej2.sh:  #!/bin/bash #SBATCH --job-name=ej2 # nombre del job. #SBATCH -c 1 # numero de cpu cores a usar. #SBATCH --time=1:00 # tiempo maximo de ejecucion del job. #SBATCH --mem-per-cpu=100mb # tamano de memoria del job en ejecucion. echo \u0026#34;Hello World” # imprime Hello World sleep 10 # duerme 10s # End of script Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej2.sh   Ejemplo 3  Muestra las variables de ambiente propias de Slurm.\n Crear el archivo ej3.sh:  #!/bin/bash #SBATCH -J ej3 # nombre del job set | grep SLURM Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej3.sh   Ejemplo 3  Muestra las variables de ambiente propias de Slurm.\n Crear el archivo ej3.sh:  #!/bin/bash #SBATCH -J ej3 # nombre del job set | grep SLURM Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej3.sh    Ejemplos de job scripts con MPI, OpenMP y Python     Ejemplo 4  Ejecución del programa en Openmpi, prueba_mpi.c:\n#include \u0026lt;mpi.h\u0026gt;#include \u0026lt;stdio.h\u0026gt; int main(int argc, char** argv) { // Initialize the MPI environment MPI_Init(NULL, NULL); // Get the number of processes int world_size; MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;world_size); // Get the rank of the process int world_rank; MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;world_rank); // Get the name of the processor char processor_name[MPI_MAX_PROCESSOR_NAME]; int name_len; MPI_Get_processor_name(processor_name, \u0026amp;name_len); // Print off a hello world message printf(\u0026#34;Hello world from processor %s, rank %d out of %d processors\\n\u0026#34;, processor_name, world_rank, world_size); // Finalize the MPI environment. MPI_Finalize(); }  Compilar el programa mpi:  [juana.perez@khipu ~]$ module load openmpi/2.1.6 [juana.perez@khipu ~]$ mpicc prueba_mpi.c -o prueba_mpi [juana.perez@khipu ~]$ module unload openmpi/2.1.6 Crear el archivo ej4.sh:  #!/bin/bash #SBATCH -J ej4 # nombre del job #SBATCH -p investigacion # nombre de la particion  #SBATCH -N 2 # numero de nodos #SBATCH --tasks-per-node=3 # numero de tasks por nodo module load openmpi/2.1.6 # carga el modulo de openmpi version 2.1.6 srun prueba_mpi # siendo prueba_mpi el nombre del programa mpi module unload openmpi/2.1.6 Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej4.sh   Ejemplo 5  Ejemplo de ejecución del programa OpenMP, prueba_openmp.c:\n#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;omp.h\u0026gt; void Hello(void); /* Thread function */ /*--------------------------------------------------------------------*/ int main(int argc, char* argv[]) { int thread_count = strtol(argv[1], NULL, 10); # pragma omp parallel num_threads(thread_count) Hello(); return 0; } /* main */ /*------------------------------------------------------------------- * Function: Hello * * Purpose: Thread function that prints message * */ void Hello(void) { int my_rank = omp_get_thread_num(); int thread_count = omp_get_num_threads(); printf(\u0026#34;Hello from thread %d of %d\\n\u0026#34;, my_rank, thread_count); /* Hello */ }  Compilar el programa prueba_openmp.c:  [juana.perez@khipu ~]$ module load gcc/5.5.0 [juana.perez@khipu ~]$ g++ prueba_openmp.c -fopenmp -lpthread -o prueba_openmp [juana.perez@khipu ~]$ module unload gcc/5.5.0 Crear el archivo ej2.sh:  #!/bin/bash #SBATCH -J ej6 #SBATCH -N 1 #SBATCH --tasks-per-node=8 #SBATCH --mem=1GB module load gcc/5.5.0 unset OMP_NUM_THREADS ./ejemplo_openmp ${SLURM_NPROCS} module unload gcc/5.5.0 Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej6.sh   Ejemplo 3  Muestra las variables de ambiente propias de Slurm.\n Crear el archivo ej3.sh:  #!/bin/bash #SBATCH -J ej3 # nombre del job set | grep SLURM Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej3.sh   Ejemplo 6  Ejecución del programa en Python, prueba_python.py:\nfrom math import factorial as f print(\u0026#34;Hola mundo\u0026#34;) N = 100 print(\u0026#34;%d! = %d\u0026#34; %(N, f(N)))  Crear el archivo ej5.sh:  #!/bin/bash #SBATCH -J ej5 # nombre del job #SBATCH -p investigacion # nombre de la particion  #SBATCH -c 1 # numero de cpu cores a usar module load python/2.7.17 # carga el modulo de python version 2.7.17 python2.7 prueba_python.py # siendo prueba_python.py el nombre del programa python module unload python/2.7.17 Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej5.sh    Ejemplos de job scripts avanzados     Ejemplo 7  Ejecución de dos jobs, donde uno depende del término del otro.\n Crear los archivos ej7-1.sh y ej7-2.sh:  #!/bin/bash #SBATCH -J ej7-1 #SBATCH -N 1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB echo \u0026#34;ej7-1\u0026#34; date sleep 120 date #!/bin/bash #SBATCH -J ej7-2 #SBATCH -N 1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB echo \u0026#34;ej7-2\u0026#34; date sleep 120 date Enviar a ejecutar los jobs:  [juana.perez@khipu ~]$ n_proc=$(sbatch ej7-1.sh) [juana.perez@khipu ~]$ sbatch -d after:${n_proc##* } ej7-2.sh [juana.perez@khipu ~]$ squeue   Ejemplo 8  Muestra como pasar variables a los jobs, esto permite setear variables de ambiente para la ejecución dentro de nuestro script. No es recomendable usar seteos de variables direcamente como comandos Unix. Existen dos formas:\n Colocar el set de variables que se necesita en el script mediante el flag “#SBATCH \u0026ndash;export=var1=valor,var2=valor…”.   Crear el archivo ej8-1.sh:  #!/bin/bash #SBATCH -J ej8-1 #SBATCH -N 1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB #SBATCH --export=var1=12 echo $var1  Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch ej8-1.sh Especificar el valor de las variables al momento del envío del script. Brinda flexibilidad y es recomendable usarlo cuando solo varían pocos parámetros, como el nombre de un archivo o valor único.   Crear el archivo ej8-2.sh:  #!/bin/bash #SBATCH -J ej8-2 #SBATCH -N 1 #SBATCH --tasks-per-node=1 #SBATCH --mem=1GB echo $var1  Enviar a ejecutar el job:  [juana.perez@khipu ~]$ sbatch --export=var1=15 ej8-2.sh    Información adicional      Tutorial de Linux Documentación de Slurm  "},{"id":5,"href":"/uso/acceso/","title":"Acceso e inicio de sesión","parent":"Uso","content":"Última actualización: 23 Agosto 2020.   Abrir un terminal y escribir el siguiente comando reemplazando su nombre de usuario en el campo .  ssh \u0026lt;username\u0026gt;@khipu.utec.edu.pe Ejemplo:\nssh juana.perez@khipu.utec.edu.pe Aparecerá el siguiente diálogo. Aceptar escribiendo yes y presionar enter. Luego colocar el password (este no aparecerá en pantalla al escribir) y presionar enter.  Are you sure you want to continue connecting (yes/no)? juana.perez@khipu.utec.edu.pe\u0026#39;s password: Al lograr accesar al cluster aparecerá el prompt del terminal remoto.  Last login: Fri Aug 21 11:28:13 2020 from 190.236.197.233 [juanaperez@khipu ~]$ Para cerrar o salir del cluster:  [juana.perez@khipu ~]$ exit Opcional: para accesar sin escribir la contraseña deben copiar la llave pública de su computador hacia el cluster. La llave pública debe haber sido creada previamente con el siguiente comando (recomendamos usar los valores por defecto presionando ENTER).  ssh-keygen Linux o MacOS:\nssh-copy-id -i ~/.ssh/id_rsa.pub juana.perez@khipu.utec.edu.pe Windows:\ntype %HOMEPATH%\\.ssh\\id_rsa.pub | ssh juana.perez@khipu.utec.edu.pe \u0026#34;mkdir .ssh; cat \u0026gt;\u0026gt; .ssh/authorized_keys\u0026#34; "},{"id":6,"href":"/software/","title":"Software","parent":"Khipu","content":""},{"id":7,"href":"/software/disponible/","title":"Software Disponible","parent":"Software","content":"Última actualización: 23 Agosto 2020.  El cluster Khipu usa CentOS 7 (Linux/GNU) como sistema operativo principal y Slurm como manager de filas. Para listar desde su terminal el software disponible para el usuario.\nmodule avail La siguiente tabla muestra los comandos para poder cargar y usar el software en su entorno.\n   Nombre Versión Comando de uso Categoría     cmake 3.16.5 module load cmake/3.16.5 compiler   gcc 5.5.0 module load gcc/5.5.0 compiler   gcc 6.5.0 module load gcc/6.5.0 compiler   gcc 7.5.0 module load gcc/7.5.0 compiler   gcc 8.4.0 module load gcc/8.4.0 compiler   gcc 9.2.0 module load gcc/9.2.0 compiler   hdf5 1.10.5 module load hdf5/1.10.5 lib   mpich 3.1.4 module load mpich/3.1.4 compiler   mpich 3.3.2 module load mpich/3.3.2 compiler   openmpi 2.1.6 module load openmpi/2.1.6 compiler   openmpi 3.1.5 module load openmpi/3.1.5 compiler   openmpi 4.0.3 module load openmpi/4.0.3 compiler   python 2.7.17 module load python/2.7.17 language   python 3.7.7 module load python/3.7.7 language   mpich 1.5 module load mpich/1.5 compiler   mpich 3.2.1 module load mpich/3.2.1 compiler   pmix 2.1 module load pmix/2.1 lib    Ejemplo\nPara cargar gcc versión 7.5.0, usar el comando de uso sugerido:\nmodule load gcc/7.5.0 Una vez terminado de el usar el software es recomendable hacer unload. Esto también se usa en scripts de jobs (ver ejemplos en la Guía de Uso).\nmodule unload gcc/7.5.0 "},{"id":8,"href":"/software/instalacion/","title":"Instalacion","parent":"Software","content":"Última actualización: 23 Agosto 2020.  Para entrar en contacto con el administrador, por favor enviar un mensaje a khipu@utec.edu.pe.\n En caso de software licenciado, el usuario debe proveer la licencia y/o instalador del software al administrador para ser instalado. El usuario puede instalar el software y libraries necesarios en su /home, en caso de no requerir licencia o ser código abierto. El usuario puede compilar en el nodo líder antes de enviar un job a ejecutar. No enviar jobs de compilación a la fila del cluster. Siendo Khipu un cluster enfocado al procesamiento y HPC; no se instalarán bases de datos, containers o máquinas virtuales. Si el software o library requiere permisos especiales para instalación, solamente el profesor encargado del curso o un investigador de proyecto pueden solicitar la instalación bajo coordinación con el administrador. En caso de necesitar libraries adicionales de Python, no crear ambientes virtuales para instalar. Solamente el profesor encargado del curso o un investigador de proyecto pueden solicitar la instalación bajo coordinación con el administrador.   Por favor entrar en contacto en caso se requiera un software o library adicional que va a ser usado por múltiples investigadores o participantes de un curso que no ha sido especificado en el Formulario de Solicitud de Acceso.\n "},{"id":9,"href":"/politica/reglas-de-uso/","title":"Reglas De Uso","parent":"Política","content":"Importante: Por favor, leer con atención.\nÚltima actualización: 23 Agosto 2020.   Las cuentas son estrictamente personales. El usuario no debe compartir su cuenta (password, ssh keys, acceso VPN). El usuario no debe usar la cuenta de otra persona, con o sin consentimiento. En caso de existir sospecha que otros han usado su cuenta, notificarlo inmediatamente a khipu@utec.edu.pe. Seguir y respetar las buenas práticas:  No usar el espacio asignado como almacenamiento personal. Usar solamente el cluster para procesamiento paralelo de acuerdo a las recomendaciones. Evitar duplicidad de archivos/directorios. Usar las aplicaciones instaladas, y compilar en su espacio personal otras aplicaciones solamente si fueran necesarias para el desarrollo de su proyecto o trabajo. No instalar software de procedencia ilegal y/o con licencia no otorgada por los desarrolladores. No ejecutar programas en el nodo de acceso (nodo líder), usar la fila.    El usuario acepta cumplir con la normativa y sanciones impuestas por UTEC en sus atribuciones. En caso de no cumplir normas de UTEC y/o atentar contra la ley, el evento será notificado a las autoridades pertinentes. En caso de incumplimiento de los items anteriores, la cuenta será suspendida.  "},{"id":10,"href":"/politica/solicitud-de-acceso/","title":"Solicitud De Acceso","parent":"Política","content":"Importante: Esta documentación es un trabajo en progreso.\nÚltima actualización: 23 Agosto 2020.  \nEl pedido de creación de cuentas para acceso lo realizará:\na) el investigador principal del proyecto (PI), o\nb) el profesor responsable del curso.\nAbrir solicitud de acceso    "},{"id":11,"href":"/politica/informacion-general/","title":"Informacion General","parent":"Política","content":"Importante: Esta documentación es un trabajo en progreso.\nÚltima actualización: 23 Agosto 2020.  El cluster Khipu es un servicio orientado a la investigación como parte del Centro de Investigación en Computación Sostenible (COMPSUST) de la Universidad de Ingeniería y Tecnología (UTEC). Si requiere más información o ayuda por favor entre en contacto a khipu@utec.edu.pe.\n"},{"id":12,"href":"/tags/","title":"Tags","parent":"Khipu","content":""}]